[{"title":"分布式系统","url":"/2023/05/01/architecture/distributed-systems/","content":"\n该文档主要是 mit 6.824 课程的笔记以及一些扩展。\n\nDistributed SystemsDrivens and Challenges分布式系统的核心是通过网络来协调，共同完成一致任务的一些计算机。包括大型网站的存储系统、大数据运算(MapReduce)。在设计一个系统时或者面对一个需要解决的问题时，如果可以在一台计算机上解决，而不需要分布式系统，那就应该用一台计算机解决问题。很多的工作都可以在一台计算机上完成，并且通常比分布式系统简单很多。所以，在选择使用分布式系统解决问题之前，应该充分尝试别的思路，因为分布式系统会让问题解决变得复杂。\n分布式系统会让问题的解决变得复杂，引入分布式系统的驱动力主要是：\n\n需要获得更高的计算性能：大量的并行运算、大量 CPU、大量内存、以及大量磁盘在并行的运行；\n可以提供容错(tolerate faults)。比如两台计算机运行完全相同的任务，其中一台发生故障，可以切换到另外一台；\n有一些问题天然在空间上就是分布式的。例如银行转账，本身就分布在不同的地域，这就需要一种两者之间协调的方法，所以有一些天然的原因导致系统是物理分布的；\n构建分布式系统来达成一些安全的目标。比如有一些代码并不被信任，但是有需要和它进行交互，这些代码不会立即表现的恶意或者出现 Bug。你不会想要信任这些代码，所以想要将代码分散在多处运行，这样你的代码在另外一台计算机运行，我的代码在我的计算机上运行，通过一些特定的网络协议通信。所以，我们可能会担心安全问题，我们把系统分成多个的计算机，这样可以限制出错域。\n\n所有的这些分布式系统的问题在于：\n\n因为系统中存在很多部分，这些部分又在并发执行，会遇到并发编程和各种复杂交互所带来的问题，以及时间依赖的问题（同步、异步）。这让分布式系统变得很难；\n分布式系统有多个组成部分，再加上计算机网络，会遇到一些意想不到的故障。如果只有一台计算机，那么它通常要么是工作，要么是故障或者没电，总的来说，要么是在工作，要么是没有工作。而由多台计算机组成的分布式系统，可能会有一部分组件在工作，而另一部分组件停止运行，或者这些计算机都正常运行，但是网络中断或者不稳定。所以，局部错误也是分布式系统很难的原因；\n人们设计分布式系统的根本原因通常是为了获得更高的性能，比如一千台计算机或者一千个磁盘能达到的性能。但是实际上一千台机器到底有多少性能是一个棘手的问题，这里有很多难点，所以通常需要加倍小心的设计才能让系统达到你期望的性能。\n\nScalability通常来说，构建分布式系统的目的是为了获取人们常常提到的可扩展的加速。所以，追求的是可扩展性。而这里说的可扩展或者可扩展性指的是，如果用一台计算机解决了一些问题，当买了第二台计算机，只需要一半的时间就可以解决这些问题。两台计算机构成的系统如果两倍性能或者吞吐，就是这里说的可扩展性。\n我们希望可以通过增加机器的方式来实现扩展，但是现实中这很难实现，需要一些架构设计来将这个可扩展性无限推进下去。\nAvailability如果只使用一台计算机构建系统，那么大概率是可靠的，因为一台计算机通常可以很好的运行很多年，计算机是可靠的，操作系统是可靠的。所以一台计算机正常工作很长时间并不少见，然而如果通过数千台计算机构建系统，对于这么多计算机，也会有很大概率的故障。所以大型分布式系统中有一个大问题，就是一些很罕见的问题会被放大。对于容错，有很多不同的概念可以表述，这些表述中，有一个共同的思想就是可用性。某些系统经过精心的设计，可以在特定的错误类型下，系统仍然能够正常运行，仍然像没有出现错误一样，提供完整的服务。\n除了可用性之外，另一种容错性是自我可恢复性(recoverability)。如果出现了问题，服务会停止工作，不再响应请求，之后有人来修复，并且在修复之后系统仍然可以正常运行，就像没有出现过问题一样。这是一个比可用性更弱的需求，因为在出现故障到故障组件被修复期间，系统将完全停止工作，但是修复之后，系统又可以完全正确的重新运行，所以可恢复性是一个重要的需求。对于一个可恢复的系统，通常需要做一些操作，例如将最新的数据存在磁盘中，这样恢复供电之后，才能将这些数据取回来等。为了实现这些特性，有很多工具。其中最重要的有两个：\n\n非易失存储（non-volatile storage，类似磁盘）：这样当出现类似的电源故障，甚至整个机房电源故障，可以使用非易失存储。可以存放一些 checkpoint 或者系统状态的 log 在这些存储中，当故障修复之后，可以从硬盘中读出系统最新的状态，并从那个状态继续运行；\n复制(replication)，不过，管理复制的多副本系统会有些棘手。任何一个多副本系统中，都会有一个关键问题，比如我们有两台服务器，本来运行着相同的系统状态，现在的关键问题在于，这两个副本总是会意外的偏离同步的状态，而不再互为副本。对于任何一种使用复制实现的容错系统，都会面临这个问题。\n\nConsistency一致性就是用来定义操作行为的概念。之所以一致性是分布式系统中一个有趣的话题，是因为，从性能和容错的角度来说，我们通常会有多个副本。在一个非分布式系统中，通常只有一个服务器，一个表单。虽然不是绝对，但是通常来说对于 put&#x2F;get 的行为不会有歧义。直观上来说，put 就是更新这个表单，get 就是从表单中获取当前表单中存储的数据，但是在分布式系统中，由于复制或者缓存，数据可能存在于多个副本当中，于是就有了多个不同版本的数据。\n实际上，对于一致性有很多不同的定义。有一些非常直观，比如说 get 请求可以得到最近一次完成的 put 请求写入的值。这种一般也被成为强一致（Strong Consistency）。但是，事实上，构建一个弱一致的系统也是非常有用的。弱一致是指不保证 get 请求可以得到最近一次完成的 put 请求写入的值。虽然强一致可以确保 get 获取的是最新的数据，但是实现这一点代价非常高，几乎可以确定的是，分布式系统的各个组件需要做大量的通信，才能实现强一致性。\nCAPCAP 理论对分布式系统的特性做了高度抽象，有三个指标：\n\nConsistency：一致性强调的不是数据完整，而是各个节点间对数据一致；\nAvailability：可用性强调的是服务可用，但不保证数据的一致性；\nPartition Tolerance：分区容错性强调的是集群对分区故障的容错能力。\n\nCAP 不可能都满足是对于一个分布式系统来说，只能在三个指标中选择其中两个。有网络交互就一定有延迟和数据丢失，而这种情况我们必须接受，还必须保证系统不能挂掉，所以 P 是必须要保证的，剩下的就只能在 C 和 A 中选一个。\nMapReduce对于一个完整的 MapReduce Job，它由 Map Task 和一些 Reduce Task 组成:\n\nJob。整个 MapReduce 计算称为 Job；\nTask。每一次 MapReduce 调用称为 Task。\n\ndevelopermasteroutputmap1map2map3map4map5reduce1reduce2job\n\nMapReduce 展示了一个分布式系统的可扩展性。\nReplication容错本身是为了提高可用性，当想构建一个服务时，尽管计算机硬件总是有可能故障，但是我们还是希望能稳定的提供服务，甚至出现了网络问题我们还是想能够提供服务。使用到的工具就是复制。最简单的方法来描述复制能处理的故障，就是单台计算机的 fail-stop 故障。不能解决软件中的 Bug 和硬件设计中的缺陷，还有另外一种情况，比如自然灾害，摧毁了整个数据中心，无论有多少副本都无济于事。如果我们想处理类似的问题，就需要将副本放在不同的城市，或者物理上把它们分开（同城双活、两地三中心架构）。另一个有关复制的问题是，这种复制的方案是否值得？这个不是一个可从技术上来回答的问题，这是一个经济上的问题，取决于一个可用服务的价值。\n\n所以任何技术都不是银弹，都是需要根据实际情况进行抉择和取舍。\n\nState Transfer状态转移背后的思想是，Primary 将自己完整状态，比如内存中的内容，拷贝并发送给 Backup。Backup 会保存收到的最近一次状态，所以 Backup 会有所有的数据。当 Primary 故障了，Backup 就可以从它所保存的最新状态开始运行。所以，状态转移就是发送Primary 的状态。\nReplicated State Machine复制状态机基于这个事实：我们想复制的大部分的服务或者计算机软件都有一些确定的内部操作，不确定的部分是外部输入。通常情况下，如果一台计算机没有外部影响，它只是一个接一个的执行命令，每条指令执行的是计算机中的内存和寄存器上的确定函数，只有当外部事件干预时，才会发生一些预期之外的事。例如某个随机事件收到了一个网络数据包，导致服务做了一些不同的事情。所以，复制状态机不会在不同的副本之间发送状态，相应的，它只会从 Primary 将这些外部事件，例如外部的输入，发送给 Backup。通常来说，如果有两台计算机，如果他们从相同的状态开始，并且它们以相同的顺序，在相同的事件，看到了相同的输入，那么它们会一直互为副本，并且保持一致。\n\n所以状态转移传输的可能是内存，而复制状态机会将来自客户端的操作或者其他外部事件，从 Primary 传输到 Backup。\n\n人们倾向于使用复制状态机的原因是，通常来说，外部操作或者事件比服务的状态要小。\nRaftSplit Brain尽管存在脑裂的可能，但是随着技术的发展，人们发现就算网络出现故障，可能出现分区，实际上是可以正确的实现能够自动完成故障切换的系统。当网络出现故障，将网络分割成两半，网络的两边独立运行，且不能访问对方，这通常被称为网络分区。在构建能自动恢复，同时又避免脑裂的多副本系统时，人们发现，关键点在于过半票决（Majority Vote）。\n过半票决系统第一步在于，服务器的数量要是奇数，而不是偶数。在任何时候为了完成任何操作，必须凑够过半的服务器来批准相应的操作。\n\n如果系统有 2*F + 1 个服务器，那么系统最多可以接受 F 个服务器出现故障，仍然可以正常工作。\n\nraft 共识算法动画演示\nAppendEntriesClientClientLeaderLeaderFollower1Follower1Follower2Follower2requestraftAppendEntriesAppendEntriesAppendEntriesAckAckWait&#31561;&#24453;&#36807;&#21322;&#33410;&#28857;&#21709;&#24212;&#65288;&#21253;&#25324;&#33258;&#24049;&#65289;&#65292;&#36825;&#37324;&#21482;&#38656;&#35201;&#31561;&#24453;&#19968;&#20010; Follower &#21709;&#24212;commitAppendEntriesjudge commit IDLeader &#20250;&#23558;&#26356;&#22823;&#30340; commit ID &#21457;&#36865;&#32473; Follower&#65292;&#24403;&#20854;&#20182;&#21103;&#26412;&#25910;&#21040;&#20102;&#36825;&#20010;&#28040;&#24687;&#23601;&#30693;&#36947;&#20043;&#21069;&#25552;&#20132;&#30340; commit &#21495;&#24050;&#32463;&#34987; Leader &#25552;&#20132;AppendEntriesjudge commit IDresponse&#31561;&#24453; commit &#36820;&#22238;&#20449;&#24687;&#65292;&#22914;&#26524;&#26377;&#36807;&#21322;&#33410;&#28857;&#21709;&#24212;&#65292;&#21017;&#36820;&#22238;&#25104;&#21151;&#65292;&#21542;&#21017;&#20250;&#37325;&#35797;\n\nLeader ElectionFollowerCandidateLeader&#21021;&#22987;&#29366;&#24577;&#36229;&#26102;&#65292;&#24320;&#22987;&#36873;&#20030;&#20986;&#29616; Leader &#25110;&#32773;&#20986;&#29616;&#26356;&#39640;&#30340;&#20219;&#26399;(term)&#36229;&#26102;&#65292;&#36827;&#34892;&#26032;&#19968;&#36718;&#36873;&#20030;&#25910;&#21040;&#36807;&#21322;&#26381;&#21153;&#30340;&#25237;&#31080;&#25910;&#21040;&#26356;&#39640;&#30340;&#20219;&#26399;(term)\n\n\n系统启动时，默认都是 Follower 状态，初始化时会随机赋予一个时间(150~300ms)，当超时之后会转换为 Candidate 状态，发起一轮投票；\nCandidate 是选主过程中的中间状态，只有大多数 Follower 投票通过时，才会转换为 Leader；\n如果 Candidate 选主超时，则会发起新一轮选主过程（当前 term 会加一）；\n如果 Candidate 收到 Leader 发来的信息时，会转换为 Follower；\nFollower 在一个任期（term）过程中，只会给一个 Candidate 投票，投票之后会重置超时时间；\nCandidate 和 Leader 在发现更大的任期时，都会转换为 Follower。\n\nElection RestrictionFollower 只能向满足下面条件之一的 Candidate 投票:\n\nCandidate 最后一条 Log 条目的任期号大于本地最后一条 Log 条目的任期号；\nCandidate 最后一条 Log 条目的任期号等于本地最后一条 Log 条目的任期号，并且 Candidate 的 Log 长度大于或等于本地 Log 记录的长度。\n\nLog BackupLeader 中的日志只能追加，Leader 会强制让 Follower 的数据和自己保持一致。Leader 为每个 Follower 维护了 nextIndex，nextIndex 的初始值是从新任的最后一条日志开始。AppendEntries 消息包含了 prevLogIndex 和 prevLogTerm 字段，这样的 AppendEntries 消息发送给了 Follower。而 Follower 它们在收到 AppendEntries 消息时，可以知道它们收到一个带有若干 Log 条目的消息，Follower 在写入 Log 之前，会检查本地的前一个 Log 条目，是否与 Leader 发来的 prevLogIndex 信息匹配。如果不匹配，Followers 会拒绝。为了响应 Follower 返回的拒绝，Leader 会减少对应的 nextIndex，直到匹配，才会把数据写入本地（新增或者修改）。\n这样一个一个回退的过程太慢了，可以让 Follower 在返回消息的时候多返回一些信息，就可以加速恢复：\n\nXTerm：这个是 Follower 中与 Leader 冲突的 Log 对应的任期号，如果 Follower 在对应位置的任期号不匹配，它会拒绝 Leader 的 AppendEntries 消息，并将自己的任期号放在 XTerm 中。如果 Follower 在对应位置没有 Log，那么会返回 -1；\nXIndex：这个是 Follower 中，对应任期号为 XTerm 的第一条 Log 条目的槽位号；\nXLen：如果 Follower 在对应位置没有 Log，那么 XTerm 会返回 -1，XLen 表示空白的 Log 槽位数。\n\n还可以通过日志快照（Log Snapshot）的方式来快速恢复：当 Follower 刚恢复，如果它的 Log 已经很滞后了，那么它会首先强制 Leader 回退自己的 Log，在某个点，Leader 将不能再回退，因为已经到了自己 Log 的起点，这个时候 Leader 会将自己的快照发给 Follower，滞后立即通过 AppendEntries 将后面的 Log 发给 Follower。\nLinearizability通常来说，线性一致等价于强一致，一个服务是线性一致的，那么它表现的就像只有一个服务器，并且服务器没有故障，这个服务器每次执行一个客户端请求，并且没有什么奇怪的事情发生。要达到线性一致性，我们现在要确定顺序，对于这个顺序，有两个限制条件：\n\n如果一个操作在另一个操作开始前就结束了，那么这个操作必须在执行历史中出现在另一个操作前面；\n执行历史中，读操作，必须在相应的 key 的写操作之后。\n\n如果我们能构建这么一个序列，那么可以证明，这里的请求历史记录是线性的，必须同时满足：\n\n序列中的请求的顺序与实际时间匹配；\n每个读请求看到的都是序列中前一个写请求写入的值。\n\nZookeeper相比 Raft 来说，Raft 实际上就是一个库。可以在更大的多副本系统中使用 Raft 库。但是 Raft 不是一个你可以直接交互的独立服务，你必须要设计你自己的应用程序来与 Raft 库交互。Zookeeper 作为一个多副本系统，是一个容错的、通用的协调服务，它与其他系统一样，通过多副本来完成容错。\nZookeeper 和 Raft 类似，先发出 Log 条目之后，当 Leader 收到了过半服务器当回复，Leader 就会发送 commit 消息。Zookeeper 的读性能随着服务器数量的增加而显著的增加。所以很明显，Zookeeper 在这里有一些修改使得读请求可以由其他副本来处理。那么 Zookeeper 是如何确保这里的读请求是安全的（线性一致）？\n实际上，Zookeeper 并不要求返回最新的写入数据。Zookeeper 的方式是，放弃线性一致性，不提供线性一致的读。所以 Zookeeper 也不用为读请求提供最新的数据，它由自己有关一致性的定义，而这个定义不是线性一致的，因此允许读请求返回旧的数据。所以 Zookeeper 这里声明自己最开始就不支持线性一致性，来解决这里的技术问题。如果不提供这个能力，那么读请求返回旧的数据。这里实际上是一种经典的解决性能和强一致之间矛盾的方法，也就是不提供强一致。\n这里的工作原理是，每个 Log 条目都会被 Leader 打上 zxid 的标签，这些标签就是 Log 对应的条目号。任何时候一个副本回复一个客户端的读请求，首先这个读请求是在 Log 的某个特定点执行的，其次回复里面会带上 zxid，对应的就是 Log 执行点的前一条 Log 条目。客户端会记住最高 zxid，当客户端发出一个请求到一个相同或者不同的副本时，它会在它的请求中带上这个最高的 zxid。这样，其他副本就知道，应该至少在 Log 中这个点或者之后执行读请求。那么在获取到对应这个位置的 Log 之前，这个副本是不能响应客户端请求。\nQuorum Replication假设有 N 个副本，为了能够执行写请求，必须要确保写操作被 W 个副本确认，W 小于 N。所以你需要将写入请求发送到这 W 个副本。如果要执行读请求，那么至少需要从 R 个副本得到所读取的信息。这里的 W 对应的数字成为 Write Quorum，R 对应的数字成为 Read Quorum。Quorum 系统要求，任意你要发送写请求的 W 个服务器，必须与任意接受读取请求的 R 个服务器由重叠。意味着，R + W 必须大于 N（至少满足 R + W = N + 1）。这样任意 W 个服务器至少与任意 R 个服务器有一个重合。\n还有一个关键点，客户端读取请求可能会得到 R 个不同的结果，需要通过最高版本号（Version）的数值作为结果。当 R 为 1 时，写请求就不再是容错的了，W 为 1 时，读请求不再是容错的，都必须要求所有的服务器在线。\n\n可以通过调整 W 和 R 来提升服务的写性能或者读性能。\n\nDistributed Transaction可以这么理解事务：程序员有一些不同的操作，或许针对数据库不同记录，他们希望所有这些操作作为一个整体，不会因为失败而被分割，也不会被其他活动看到中间状态。事务处理系统要求程序员对这些读操作、写操作标明起始和结束，这样才能知道事务起始和结束。事务处理系统可以保证在事务的开始和结束之间的行为是可预期的。数据库通常对于正确性有一个概念称为 ACID:\n\nAtomic，原子性。意味着事务可能有多个步骤，比如写多个数据记录，尽管可能存在故障，但是要么所有的写数据都完成了，要么没有写数据能完成。不应该发生类似的这种情况：在一个特定的时间发生了故障，导致事务中一半的写数据完成并可见，另一半的写数据没有完成，这里要么全有，要么全没有（All or Nothing）；\nConsistent，一致性。它通常是指数据库会强制某些应用程序定义的数据不变；\nIsolated，隔离性。这是一个属性，表明两个同时运行的事务，在事务结束前，能不能看到彼此的更新，能不能看到另一个事务中间的临时的更新。目标是不能，隔离在技术上的具体体现是，事务需要串行执行。事务不能看到彼此之间的中间状态，只能看到完成的事务结果；\nDurable，持久化。意味着在事务提交之后，数据库中的修改是持久化的，不会应为一些错误而被擦除。这意味着数据要被写入到一些非易失的存储（Non-Volatile Storage），持久化的存储，例如磁盘。\n\n通常来说，隔离性意味着可序列化（Serializable）。它的定义是如果在同一时间并行的事务，那么可以生成一系列的结果。这里的结果包含：\n\n由任何事务中的修改行为产生的数据库记录的修改；\n任何事务生成的输出。\n\n我们说可序列化是指，并行的执行一些事务得到的结果，与按照某种串行的顺序来执行这些事务，可以得到相同的结果。实际的执行过程或许有大量的并行处理，但是这里要求得到的结果与按照某种顺序一次一个事务的串行执行结果是一样的。所以，如果检查一个并发事务执行是否是可序列化的，可以查看结果，并看看是否可以找到对于同一些事务，存在一次只执行一个事务的顺序，按照这个顺序执行可以生成相同的结果。\n\n现实中隔离性要看数据库配置的隔离级别: 数据库事务隔离级别。\n\nConcurrency Control在并发控制中，主要有两种策略:\n\n悲观并发控制（Pessimistic Concurrency Control）: 在事务使用任何数据之前，它需要获得数据的锁，如果有一些其他的事务已经在使用这里的数据，锁会被它们持有，当前事务必须等待这些事务结束，之后当前事务才能获取到锁。在悲观系统中，如果由锁冲突，就会造成延时等待；\n乐观并发控制（Optimistic Concurrency Control）: 基本思想是，你不用担心其他事务是否正在读写你要使用的数据，你直接继续执行你的读写操作，通常来说这些执行会在一些临时区域，只有在事务最后的时候，再检查是不是有一些其他事务干扰了你。如果没有就可以完成事务，并且不需要承受锁带来的性能损耗，因为操作锁的代价一般都比较高；如果有一些其他的事务在同一时间修改了你关心的数据，造成了冲突，那么就必须 Abort 掉当前事务，并重试。\n\n具体使用哪种策略应该取决于不同的环境，如果冲突非常频繁，或许用悲观并发控制更好一些。悲观控制的锁就是两阶段锁（Two-Phase Locking）。\nTwo-Phase CommitClientClientTCTCAABBCCoperatorgetsetsetmulti operatorpreparePrepareYes/NoPrepareYes/NoPrepareYes/Noalt[&#25152;&#26377;&#30340;&#32467;&#26524;&#37117;&#36820;&#22238; Yes]commitCommit(WAL)Response CommitloopCommitAckloopCommitAckloopCommitAck[&#33267;&#23569;&#26377;&#19968;&#20010;&#32467;&#26524;&#36820;&#22238; No]abortAbort(WAL)Response AbortloopAbortAckloopAbortAckloopAbortAck\n\n有一些关键点：\n\n一旦回复 Prepare 消息为 Yes 之后，就不能结束事务，必须等待 TC 进行协调；回复 No 之后可以直接 Abort 掉本地事务;\n本地没有对应的 Abort 事务也要返回 Ack 信息。\n\n\n参考链接:\n\nhttps://mit-public-courses-cn-translatio.gitbook.io/mit6-824\nhttp://www.kailing.pub/raft/index.html\nhttps://sineyuan.github.io/post/etcd-raft-source-guide/\nhttps://github.com/etcd-io/raft\nhttps://raft.github.io/\nhttps://github.com/goraft/raft\nhttps://github.com/hashicorp/raft\nhttps://zhuanlan.zhihu.com/p/49792009\nhttp://www.zhaowenyu.com/etcd-doc/introduction/what-is-raft.html\nhttps://zhuanlan.zhihu.com/p/91288179\nhttps://docs.qq.com/doc/DY0VxSkVGWHFYSlZJ?_t&#x3D;1609557593539\nhttps://www.open-open.com/lib/view/open1328763454608.html\nhttps://ms2008.github.io/2019/12/04/etcd-rumor/\nhttps://zhuanlan.zhihu.com/p/152105666\nhttps://zhuanlan.zhihu.com/p/524885008\nhttps://t1mek1ller.github.io/2018/03/01/raft/\nhttps://www.jianshu.com/p/ce47091ccd5b\nhttps://cloud.tencent.com/developer/beta/article/1833688\nhttps://cloud.tencent.com/developer/beta/article/1450773\n\n","categories":["Architecture"],"tags":["Distributed","Raft"]},{"title":"eBPF 原理及其应用","url":"/2022/12/06/eBPF/getting-start-with-eBPF/","content":"\n该文档主要是《eBPF 核心技术与实战》课程的笔记以及一些扩展。其中代码在ebpf-learn-code仓库中，编译环境都在 Docker 镜像中，也具备 cilium-ebpf 的编译环境。\n\n什么是eBPFeBPF 是从 BPF 技术扩展而来的，得益于 BPF 的设计:\n\n内核态引入一个新的虚拟机(执行引擎)，所有指令都在内核虚拟机中运行；\n用户态使用 BPF 字节码来定义过滤表达式，然后传递给内核，由内核虚拟机解释执行。\n\n这就使得包过滤可以直接在内核中执行，避免了向用户态复制每个数据包，从而极大提升了包过滤的性能，进而被广大操作系统广泛接受。而 BPF 最初的名字由最初的 BSD Packet Filter 变成了 Berkeley Packet Filter 。\n发展历程\nBPF 诞生五年后， Linux 2.1.75 首次引入了 BPF 技术;\nLinux 3.0 中增加的 BPF 即时编译器替换了原本性能较差的解释器，算是一个重大更新;\n2014 年，为了研究新的SDN(软件定义网络)方案，第一次革命性的更新，将 BPF 扩展为一个通用的虚拟机，也就是 eBPF。不仅扩展了寄存器数量，引入了全新的 BPF 映射存储(Map)，还在 5.x 内核中将原本单一的数据包过滤事件逐步扩展到了内核态函数、用户态函数、跟踪点、性能事件(perf_events)以及安全控制等(转折点)。\n\n工作原理eBPF 程序并不像常规的线程那样，启动后就一直运行在那里，它需要事件触发后才会执行。这些事件包括系统调用、内核跟踪点、内核函数和用户态函数的调用退出、网络事件等等，借助强大的内核态插桩(kprobe)和用户态插桩(uprobe)，eBPF 程序几乎可以在内核和应用的任意位置进行插桩。\n\n确保安全和稳定一直都是 eBPF 的首要任务，不安全的 eBPF 程序根本就不会提交到内核虚拟机中执行。\n\n执行过程如下图（图片来自www.brendangregg.com）:\n\n通常借助 LLVM 把编写的 eBPF 程序转换为 BPF 字节码，然后再通过 bpf 系统调用提交给内核执行。内核在接受 BPF 字节码之前，会首先通过验证器对字节码进行校验，只有通过校验的 BPF 字节码才会提交到即时编译器(JIT)执行。如果 BPF 字节码中包含了不安全的操作，验证器会直接拒绝 BPF 程序的执行：\n\n只有特权进程才可以执行 bpf 系统调用;\nBPF 程序不能包涵无限循环;\nBPF 程序不能导致内核崩溃;\nBPF 程序必须在有限时间内完成。\n\nBPF 程序可以利用 BPF 映射进行存储，而用户态程序通常也需要通过 BPF 映射同运行在内核中的 BPF 程序进行交互。如下图(ebpf.io):\n\n限制eBPF 不是万能的，有很多限制：\n\neBPF 程序必须被验证器校验通过后才能执行，且不能包含无法到达的指令;\neBPF 程序不能随意调用内核函数，只能调用在 API 中定义的辅助函数;\neBPF 程序栈空间最多只有 512 字节，想要更大的存储，就必须要借助映射存储(Map);\n在内核 5.2 之前，eBPF 字节码最多只支持 4096 条指令，而 5.2 内核把这个限制提高到了 100w 条;\n由于内核的快速变化，在不同版本内核中运行时，需要访问内核数据结构的 eBPF 程序可能需要调整源码，并重写编译。\n\n环境搭建作为 eBPF 最重大的改进之一，一次编译到处执行(CO-RE)解决了内核数据结构在不同版本差异导致的兼容性问题，不过在使用 CO-RE 之前，需要内核开启 CONFIG_DEBUG_INFO_BTF=y 和 CONFIG_DEBUG_INFO=y 这两个编译选项。高版本已经默认开启，否则需要重新编译内核。\n开发工具主要包括:\n\n将 eBPF 程序编译成字节码的 LLVM;\nC 语言程序编译工具 make;\n最流行的 eBPF 工具集 BCC 和它依赖的内核头文件;\n与内核代码仓库实时同步的 libbpf;\n同样是内核代码提供的 eBPF 程序管理工具 bpftool。\n\n可以通过下面的命令来统一安装:\n# For Ubuntu20.10+sudo apt-get install -y  make clang llvm libelf-dev libbpf-dev bpfcc-tools libbpfcc-dev linux-tools-$(uname -r) linux-headers-$(uname -r)# For RHEL8.2+sudo yum install libbpf-devel make clang llvm elfutils-libelf-devel bpftool bcc-tools bcc-devel\n\n也可以直接通过 https://github.com/champly/ebpf-learn-code 这个项目里面的环境来学习和测试，直接 make run 就可以了。\n开发一般来说，eBPF的开发和执行过程分为以下 5 步:\n\n使用 C 语言开发一个 eBPF 程序;\n借助 LLVM 把 eBPF 程序编译成 BPF 字节码;\n通过 bpf 系统调用，把 BPF 字节码提交给内核;\n内核验证并运行 BPF 字节码，并把相应的程序状态保存到 BPF 映射中;\n用户程序通过 BPF 映射查询 BPF 字节码的运行状态。\n\nBCCBCC 是一个 BPF 编译器集合，包含了用于构建 BPF 程序的变成框架和库，并提供了大量可以直接使用的工具。把上述的 eBPF 执行过程通过内置框架抽象了起来，并提供了 Python、C++ 等编程语言的接口，就可以简单通过 Python 语言去跟 eBPF 的各种事件和数据进行交互。\n如下这个代码:\ntrace-open.c#include &lt;linux/sched.h&gt;#include &lt;uapi/linux/openat2.h&gt;struct data_t &#123;  u32 pid;  u64 ts;  char comm[TASK_COMM_LEN];  char fname[NAME_MAX];&#125;;BPF_PERF_OUTPUT(events);// define kprobe functionint hello_world2(struct pt_regs *ctx, int dfd, const char __user *filename,                 struct open_how *how) &#123;  struct data_t data = &#123;&#125;;  // get PID &amp; time  data.pid = bpf_get_current_pid_tgid();  data.ts = bpf_ktime_get_ns();  // get program name  if (bpf_get_current_comm(&amp;data.comm, sizeof(data.comm)) == 0) &#123;    bpf_probe_read(&amp;data.fname, sizeof(data.fname), (void *)filename);  &#125;  // submit event  events.perf_submit(ctx, &amp;data, sizeof(data));  return 0;&#125;\n\ntrace-open.pyfrom bcc import BPF# 1) load BPF programb = BPF(src_file=&quot;trace-open.c&quot;)b.attach_kprobe(event=&quot;do_sys_openat2&quot;, fn_name=&quot;hello_world2&quot;)# 2) print headerprint(&quot;%-18s %-16s %-6s %-16s&quot; % (&quot;TIME(s)&quot;, &quot;COMM&quot;, &quot;PID&quot;, &quot;FILE&quot;))# 3) define the callback for perf eventstart = 0def print_event(cpu, data, size):    global start    event = b[&quot;events&quot;].event(data)    if start == 0:        start = event.ts    time_s = (float(event.ts - start)) / 1000000000    print(&quot;%-18.9s %-16s %-6d %16s&quot; % (time_s, event.comm, event.pid, event.fname))# 4) loop with callback to print_eventb[&quot;events&quot;].open_perf_buffer(print_event)while 1:    try:        b.perf_buffer_poll()    except KeyboardInterrupt:        exit()\n\n打开一个终端:\nTerminal1$ make runsh docker-run.shroot@9ceb0649d5e5:/# cd ~/code/session3/root@9ceb0649d5e5:~/code/session3# python3 trace-open.pyTIME(s)            COMM             PID    FILE\n\n打开另外一个终端:\nTerminal2# champly @ champlydeiMac in ~ [15:40:16]$ docker exec -it ebpf-for-mac bashroot@9ceb0649d5e5:/# lsbin  boot  dev  etc  home  lib  lib32  lib64  libx32  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  varroot@9ceb0649d5e5:/#\n\n同时可以看到第一个终端有数据输出:\nTerminal1TIME(s)            COMM             PID    FILE0.0                b&#x27;ls&#x27;            8931   b&#x27;/etc/ld.so.cache&#x27;0.0006603          b&#x27;ls&#x27;            8931   b&#x27;/lib/x86_64-linux-gnu/libselinux.so.1&#x27;0.0007822          b&#x27;ls&#x27;            8931   b&#x27;/lib/x86_64-linux-gnu/libc.so.6&#x27;0.0008942          b&#x27;ls&#x27;            8931   b&#x27;/lib/x86_64-linux-gnu/libpcre2-8.so.0&#x27;0.0011795          b&#x27;ls&#x27;            8931   b&#x27;/proc/filesystems&#x27;0.0013228          b&#x27;ls&#x27;            8931               b&#x27;.&#x27;\n\n可以在第二个终端通过 bpftool 来查看 eBPF 程序的状态:\nTerminal2root@9ceb0649d5e5:/# bpftool prog list3: cgroup_device  tag 531db05b114e9af3\tloaded_at 2022-12-02T01:02:16+0000  uid 0\txlated 512B  jited 325B  memlock 4096B\t......155: kprobe  name hello_world2  tag c3d700bdee3931e4  gpl\tloaded_at 2022-12-06T07:41:43+0000  uid 0\txlated 528B  jited 359B  memlock 4096B  map_ids 296\tbtf_id 28root@9ceb0649d5e5:/# bpftool map list17: ringbuf  name blocked_packets  flags 0x0\tkey 0B  value 0B  max_entries 16777216  memlock 0B18: hash  name allowed_map  flags 0x0\tkey 4B  value 4B  max_entries 10000  memlock 81920B20: lpm_trie  name allowed_trie  flags 0x1\tkey 8B  value 8B  max_entries 1024  memlock 16384B297: perf_event_array  name events  flags 0x0\tkey 4B  value 4B  max_entries 1  memlock 4096Broot@9ceb0649d5e5:/# bpftool map dump id 297key:00 00 00 00value:Unknown error 524Found 0 elementsroot@9ceb0649d5e5:/# bpftool prog dump xlated id 155int hello_world2(struct pt_regs * ctx):; int hello_world2(struct pt_regs *ctx) &#123;   0: (bf) r6 = r1; int dfd = ctx-&gt;di; const char __user *filename = ctx-&gt;si; struct open_how *how = ctx-&gt;dx;   1: (79) r7 = *(u64 *)(r6 +104)   2: (b7) r1 = 0; struct data_t data = &#123;&#125;;   3: (7b) *(u64 *)(r10 -8) = r1   4: (7b) *(u64 *)(r10 -16) = r1   5: (7b) *(u64 *)(r10 -24) = r1   6: (7b) *(u64 *)(r10 -32) = r1   7: (7b) *(u64 *)(r10 -40) = r1   8: (7b) *(u64 *)(r10 -48) = r1   9: (7b) *(u64 *)(r10 -56) = r1  10: (7b) *(u64 *)(r10 -64) = r1  11: (7b) *(u64 *)(r10 -72) = r1  12: (7b) *(u64 *)(r10 -80) = r1  13: (7b) *(u64 *)(r10 -88) = r1  14: (7b) *(u64 *)(r10 -96) = r1  15: (7b) *(u64 *)(r10 -104) = r1  16: (7b) *(u64 *)(r10 -112) = r1  17: (7b) *(u64 *)(r10 -120) = r1  18: (7b) *(u64 *)(r10 -128) = r1  19: (7b) *(u64 *)(r10 -136) = r1  20: (7b) *(u64 *)(r10 -144) = r1  21: (7b) *(u64 *)(r10 -152) = r1  22: (7b) *(u64 *)(r10 -160) = r1  23: (7b) *(u64 *)(r10 -168) = r1  24: (7b) *(u64 *)(r10 -176) = r1  25: (7b) *(u64 *)(r10 -184) = r1  26: (7b) *(u64 *)(r10 -192) = r1  27: (7b) *(u64 *)(r10 -200) = r1  28: (7b) *(u64 *)(r10 -208) = r1  29: (7b) *(u64 *)(r10 -216) = r1  30: (7b) *(u64 *)(r10 -224) = r1  31: (7b) *(u64 *)(r10 -232) = r1  32: (7b) *(u64 *)(r10 -240) = r1  33: (7b) *(u64 *)(r10 -248) = r1  34: (7b) *(u64 *)(r10 -256) = r1  35: (7b) *(u64 *)(r10 -264) = r1  36: (7b) *(u64 *)(r10 -272) = r1  37: (7b) *(u64 *)(r10 -280) = r1  38: (7b) *(u64 *)(r10 -288) = r1; data.pid = bpf_get_current_pid_tgid();  39: (85) call bpf_unspec#0; data.pid = bpf_get_current_pid_tgid();  40: (63) *(u32 *)(r10 -288) = r0; data.ts = bpf_ktime_get_ns();  41: (85) call bpf_unspec#0; data.ts = bpf_ktime_get_ns();  42: (7b) *(u64 *)(r10 -280) = r0; struct data_t data = &#123;&#125;;  43: (bf) r1 = r10  44: (07) r1 += -272; if (bpf_get_current_comm(&amp;data.comm, sizeof(data.comm)) == 0) &#123;  45: (b7) r2 = 16  46: (85) call bpf_unspec#0  47: (67) r0 &lt;&lt;= 32  48: (77) r0 &gt;&gt;= 32; if (bpf_get_current_comm(&amp;data.comm, sizeof(data.comm)) == 0) &#123;  49: (55) if r0 != 0x0 goto pc+5; bpf_probe_read(&amp;data.fname, sizeof(data.fname), (void *)filename);  50: (bf) r1 = r10  51: (07) r1 += -256; bpf_probe_read(&amp;data.fname, sizeof(data.fname), (void *)filename);  52: (b7) r2 = 255  53: (bf) r3 = r7  54: (85) call bpf_unspec#0; bpf_perf_event_output(ctx, bpf_pseudo_fd(1, -1), CUR_CPU_IDENTIFIER, &amp;data, sizeof(data));  55: (18) r2 = map[id:297]  57: (bf) r4 = r10  58: (07) r4 += -288; bpf_perf_event_output(ctx, bpf_pseudo_fd(1, -1), CUR_CPU_IDENTIFIER, &amp;data, sizeof(data));  59: (bf) r1 = r6  60: (18) r3 = 0xffffffff  62: (b7) r5 = 288  63: (85) call bpf_unspec#0; return 0;  64: (b7) r0 = 0  65: (95) exit\n\n可以查看到刚运行的 eBPF 程序的状态和 map 中的值，还有 eBPF 程序指令，通过 bpftool prog dump jited id $ID 可以查看到最后经过 JIT 编译之后的指令(当前 Docker 环境不支持，如果可以支持，请告诉我)。\n还可以通过 strace 来查看 eBPF 程序的执行过程:\nroot@9ceb0649d5e5:~/code/session3# strace -v -f -ebpf ./hello.py......bpf(    BPF_PROG_LOAD,     &#123;        prog_type=BPF_PROG_TYPE_KPROBE,         insn_cnt=13,         insns=[            &#123;code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0x21646c72&#125;,             &#123;code=BPF_STX|BPF_W|BPF_MEM, dst_reg=BPF_REG_10, src_reg=BPF_REG_1, off=-8, imm=0&#125;,             &#123;code=BPF_LD|BPF_DW|BPF_IMM, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0x6c6c6548&#125;,             &#123;code=BPF_LD|BPF_W|BPF_IMM, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0x6f57206f&#125;,             &#123;code=BPF_STX|BPF_DW|BPF_MEM, dst_reg=BPF_REG_10, src_reg=BPF_REG_1, off=-16, imm=0&#125;,             &#123;code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0&#125;,             &#123;code=BPF_STX|BPF_B|BPF_MEM, dst_reg=BPF_REG_10, src_reg=BPF_REG_1, off=-4, imm=0&#125;,             &#123;code=BPF_ALU64|BPF_X|BPF_MOV, dst_reg=BPF_REG_1, src_reg=BPF_REG_10, off=0, imm=0&#125;,             &#123;code=BPF_ALU64|BPF_K|BPF_ADD, dst_reg=BPF_REG_1, src_reg=BPF_REG_0, off=0, imm=0xfffffff0&#125;,             &#123;code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_2, src_reg=BPF_REG_0, off=0, imm=0xd&#125;,             &#123;code=BPF_JMP|BPF_K|BPF_CALL, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0x6&#125;,             &#123;code=BPF_ALU64|BPF_K|BPF_MOV, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0&#125;,             &#123;code=BPF_JMP|BPF_K|BPF_EXIT, dst_reg=BPF_REG_0, src_reg=BPF_REG_0, off=0, imm=0&#125;        ],         license=&quot;GPL&quot;,         log_level=0,         log_size=0,         log_buf=NULL,         kern_version=KERNEL_VERSION(5, 15, 49),         prog_flags=0,         prog_name=&quot;hello_world&quot;,         prog_ifindex=0,         expected_attach_type=BPF_CGROUP_INET_INGRESS,         prog_btf_fd=3,         func_info_rec_size=8,         func_info=0x55d9d310c030,         func_info_cnt=1,         line_info_rec_size=16,         line_info=0x55d9d30ebf10,         line_info_cnt=5,         attach_btf_id=0,         attach_prog_fd=0,         fd_array=NULL    &#125;, 144) = 4\n\n如果去掉 -ebpf 可以看到更多的过程。\n内核调用对于用户态程序来说，它们与内核交互时必须要通过系统调用来完成。而对于 eBPF 程序需要通过 bpf 系统调用 man bpf:\nNAME       bpf - perform a command on an extended BPF map or programSYNOPSIS       #include &lt;linux/bpf.h&gt;       int bpf(int cmd, union bpf_attr *attr, unsigned int size);\n\ninclude&#x2F;uapi&#x2F;linux&#x2F;bpf.h 可以查看支持的 BPF 命令。\nBPF 命令\n\n\nBPF命令\n功能描述\n\n\n\nBPF_MAP_CREATE\n创建一个 BPF 映射\n\n\nBPF_MAP_LOOKUP_ELEMBPF_MAP_UPDATE_ELEMBPF_MAP_DELETE_ELEMBPF_MAP_LOOKUP_AND_DELETE_ELEMBPF_MAP_GET_NEXT_KEY\nBPF 映射相关的操作命令，包括查找、更新、删除以及遍历等\n\n\nBPF_PROG_LOAD\n验证并加载 BPF 程序\n\n\nBPF_PROG_ATTACH\n把 BPF 程序挂载到内核事件上\n\n\nBPF_PROG_DETACH\n把 BPF 程序从内核事件上卸载\n\n\nBPF_OBJ_PIN\n把 BPF 程序或映射挂载到sysfs中的&#x2F;sys&#x2F;fs&#x2F;bpf目录中 （常用于保持 BPF 程序在内核中贮存）\n\n\nBPF_OBJ_GET\n从&#x2F;sys&#x2F;fs&#x2F;bpf目录中查找 BPF 程序 BPF_BTF_LOAD 验证并加载 BTF 信息\n\n\nBPF 辅助函数可以通过 bpftool feature probe 来查询当前系统支持的辅助函数列表，详细定义可以使用 man bpf-helpers，或者参考内核头文件 &#x2F;include&#x2F;uapi&#x2F;linux&#x2F;bpf.h:\n\n\n\n辅助函数\n功能描述\n\n\n\nbpf_trace_printk(fmt, fmt_size, …)\n向调试文件系统写入调试信息\n\n\nbpf_map_lookup_elem(map, key)bpf_map_update_elem(map, key, value, flags)bpf_map_delete_elem(map, key)\nBPF映射操作函数，分别是查找、更新和删除元素\n\n\nbpf_probe_read(dst, size, ptr)bpf_probe_read_user(dst, size, ptr)bpf_probe_read_kernel(dst, size, ptr)\n从内存指针中读取数据从用户空间内存指针中读取数据从内核空间内存指针中读取数据\n\n\nbpf_ktime_get_ns()\n获取系统启动以来的时长，单位纳秒\n\n\nbpf_get_current_pid_tgid()\n获取当前线程的TGID（高32位）和PID（低32位）\n\n\nbpf_get_current_comm(buf, size)\n获取当前线程的任务名称\n\n\nbpf_get_current_task()\n获取当前任务的task结构体\n\n\nbpf_perf_event_output(ctx, map, flags, data, size)\n向性能事件缓冲区中写入数据\n\n\nbpf_get_stackid(ctx, map, flags)\n获取内核态和用户态调用栈\n\n\nBPF 映射BPF 映射用于提供大块的键值存储，这些存储可被用户控件程序访问，进而获取 ePBF 程序的运行状态。eBPF 程序最多可以访问 64 个不同的 BPF 映射，并且不同的 eBPF 程序也可以通过相同的 BPF 映射来共享它们的状态。\n\nBPF 映射只能通过用户态程序的系统调用来创建，内核头文件 &#x2F;include&#x2F;uapi&#x2F;linux&#x2F;bpf.h 中的 bpf_map_type 定义了所有支持的映射类型，也可以通过 bpftool feature probe | grep map_type 来查询当前系统支持哪些映射。\n\n\n\n映射类型\n功能描述\n\n\n\nBPF_MAP_TYPE_HASH\n哈希表映射，用于保存 key&#x2F;value对\n\n\nBPF_MAP_TYPE_LRU_HASH\n类似于哈希表映射，但在表满的时候自动按LRU 算法删除最久未被使用的元素\n\n\nBPF_MAP_TYPE_ARRAY\n数组映射，用于保存固定大小的数组（注意数组元素无法删除）\n\n\nBPF_MAP_TYPE_PROG_ARRAY\n程序数组映射，用于保存 BPF 程序的引用，特别适合于尾调用（即调用其他 eBPF 程序)\n\n\nBPF_MAP_TYPE_PERF_EVENT_ARRAY\n性能事件数组映射，用于保存性能事件跟踪记录\n\n\nBPF_MAP_TYPEPERCPU_HASHBPF_MAP_TYPE_PERCPU_ARRAY\n每个 CPU 单独维护的哈希表和数组映射\n\n\nBPF_MAP_TYPE_STACK_TRACE\n调用栈跟踪映射，用于存储调用栈信息\n\n\nBPF_MAP_TYPE_ARRAY_OF_MAPSBPF_MAP_TYPE_HASH_OF_MAPS\n映射数组和映射哈希，用于保存其他映射的引用\n\n\nBPF_MAP_TYPE_CGROUP_ARRAY\nCGROUP 数组映射，用于存储 cgroups 引用\n\n\nBPF_MAP_TYPE_SOCKMAP\n套接字映射，用于存储套接字引用，特别适用于套接字重定向\n\n\n\n** BPF 映射会在用户态程序关闭文件描述符的时候自动删除**，如果想在程序退出后还保留映射，需要调用 BPF_OBJ_PIN 命令，将映射挂载到 /sys/fs/bpf 中。\n\n通过 bpftool 来查看或操作映射的具体内容:\nroot@76f664f7d01b:~/code/session3# bpftool map create /sys/fs/bpf/stats_map type hash key 2 value 2 entries 8 name stats_maproot@76f664f7d01b:~/code/session3# bpftool map17: ringbuf  name blocked_packets  flags 0x0\tkey 0B  value 0B  max_entries 16777216  memlock 0B18: hash  name allowed_map  flags 0x0\tkey 4B  value 4B  max_entries 10000  memlock 81920B20: lpm_trie  name allowed_trie  flags 0x1\tkey 8B  value 8B  max_entries 1024  memlock 16384B304: hash  name stats_map  flags 0x0\tkey 2B  value 2B  max_entries 8  memlock 4096Broot@76f664f7d01b:~/code/session3# bpftool map update name stats_map key 0xc1 0xc2 value 0xa1 0xa2root@76f664f7d01b:~/code/session3# bpftool map dump name stats_mapkey: c1 c2  value: a1 a2Found 1 elementroot@76f664f7d01b:~/code/session3# rm /sys/fs/bpf/stats_maproot@76f664f7d01b:~/code/session3#\n\nBPF 类型格式(BTF)开发 eBPF 程序时最常碰到的问题：内核数据结构的定义。安装 BCC 工具的时候会安装 linux-headers-$&#123;uname -r&#125; 依赖项。因为 BCC 在编译的时候需要从内核头文件中找到相应的内核数据结构定义。但是编译时依赖内核头文件也会带来很多问题:\n\neBPF 程序开发的时候为了获得内核数据结构的定义，就需要引入一大堆的内核头文件;\n内核头文件的路径和数据结构定义在不同内核版本中很可能不同。因此，在升级内核版本时，就会遇到找不到头文件和数据结构定义错误的问题;\n在很多生产环境的机器中，出于安全考虑，并不允许安装内核头文件，这时就无法得到内核数据结构的定义。在程序中定义数据结构虽然可以暂时解决问题，但也很容易把使用着错误数据结构的 eBPF 程序带入新版本内核中运行。\n\nBPF 类型格式(BPF Type Format,BTF) 的诞生就是为了解决这些问题。从内核 5.2 开始，只要开启了 CONFIG_DEBUG_INFO_BTF，在编译内核时，内核数据结构的定义就会自动内嵌在内核二进制文件 vmlinux 中。可以借助下面的命令把这些数据结构的定义导出到一个头文件中 vmlinux.h。\nbpftool btf dump file /sys/kernel/btf/vmlinux format c &gt; vmlinux.h\n\n解决了内核数据结构的定义问题，接下来就是如何让eBPF程序在升级内核之后，不需要重新编译就可以直接运行(CO-RE)，eBPF 的一次编译到处执行项目借助了 BTF 提供的调试信息，再通过下面两个步骤，使得 eBPF 程序可以适配不同版本的内核:\n\n通过对 BPF 代码中的访问偏移量进行重写，解决了不同内核版本中数据结构偏移量不同的问题;\n在 libbpf 中预定不同内核版本中的数据结构的修改，解决了不同内核数据结构不兼容的问题。\n\n分类跟踪类主要用于从系统中提取跟踪信息，进而为监控、排错、性能优化等提供数据支撑:\n\n\n\n程序类型\n功能描述\n功能限制\n\n\n\nBPF_PROG_TYPE_KPROBE\n用于对特定函数进行动态插桩，根据函数位置的不同，又可以分为内核态 kprobe 和用户态 uprobe\n内核函数和用户函数的定义属于不稳定 API，在不同内核版本中使用时，可能需要调整 eBPF 代码实现\n\n\nBPF_PROG_TYPETRACEPOINT\n用于内核静态跟踪点（可以 使用 perf list 命令，查询所 有的跟踪点）\n虽然跟踪点可以保持稳定性，但不如 KPROBE 类型 灵活，无法按需增加新的跟踪点\n\n\nBPF_PROG_TYPE_PERF_EVENT\n用于性能事件 (perf_events）跟踪，包括 内核调用、定时器、硬件等 各类性能数据\n需配合 BPF_MAP_TYPE_PERF_EVENT_ARRAY 或 BPF_MAP_TYPE_RINGBUF 类型的映射使用\n\n\nBPF_PROG_TYPE_RAW_TRACEPOINTBPF_PROG_TYPE_RAW_TRACEPOINT_WRITABLE\n用于原始跟踪点\n不处理参数\n\n\nBPF_PROG_TYPE_TRACING\n用于开启 BTF 的跟踪点 需要开启 BTF\n\n\n\n网络类主要用于对网络数据包进行过滤和处理，进而实现网络对观测、过滤、流量控制以及性能优化等各种丰富的功能。根据事件触发的位置不同，又可以分为 XDP(eXpress Data Path, 高速数据路径)程序、TC(Traffic Control, 流量控制)程序、套接字程序以及 cgroup 程序:\nXDP在网络驱动程序刚刚收到数据包时触发执行。由于无需通过繁杂的内核网络协议栈，XDP 程序可以用来实现高性能的网络处理方案，常用于 DDos 防御、防火墙、4 层负载均衡等场景。根据网卡和网卡驱动是否是原生支持 XDP 程序，XDP 运行模式可以分为:\n\n通用模式：不需要网卡和网卡驱动支持，XDP 程序像常规的网络协议栈一样运行在内核中，性能相对较差，一般用于测试;\n原生模式：需要网卡驱动支持，XDP 程序在网卡驱动程序的早期路径运行;\n卸载模式：需要网卡固件支持 XDP 卸载，XDP 程序直接运行在网卡上，而不再需要消耗主机的 CPU 资源，具有最好的性能。\n\n不管什么模式，XDP 程序在处理过网络包之后，都需要根据 eBPF 程序执行的结果决定数据包的去处：\n\n\n\n结果码\n含义\n使用场景\n\n\n\nXDP_DROP\n丢包\n数据包尽早丢弃可以减少 CPU 处理时间，因而常用于防火墙、DDos 防御等丢弃非法包的场景\n\n\nXDP_PASS\n传递到内核协议栈\n内核协议栈接收到网络包，按正常流程继续处理\n\n\nXDP_TXXDP_REDIRECT\n转发数据包到同一网卡&#x2F;不同网卡\n数据包在 XDP 程序修改后转发到网卡中，继续按正常的内核协议栈流程处理，常用在负载均衡中\n\n\nXDP_ABORTED\n错误\nXDP 程序运行错误，数据包丢弃并记录错误行为，以便排错\n\n\n\n只能用于接收!\n\nTC定义为 BPF_PROG_TYPE_SCHED_CLS 和 BPF_PROG_TYPE_SCHED_ACT，分别作为 Linux 流量控制的分类器和执行器。得益于内核 4.4 引入的 direct-action 模式，TC 程序可以直接在一个程序内完成分类和执行的动作，无需再调用其他的 TC 排队规则和分类器。\n同 XDP 程序相比，TC 程序可以直接获取内核解析后的网络报文数据结构 sk_buff (XDP 是 xdp_buff)，并且可以在网卡的接收和发送两个方向上执行:\n\n对于接收网络包，TC 程序在网卡接收(GRO)之后、协议栈处理(包括 IP 层处理和 iptables 等)之前执行;\n对于发送的网络包，TC 程序在协议处理（包括 IP 层处理和 iptables 等）之后、数据包发送到网卡队列(GSO)之前执行。\n\n\n套接字用于过滤、观测或重定向套接字网络包，根据类型的不同，可以挂载到套接字(socket)、控制组(cgroup)以及网络命名空间(netns)等各个位置:\n\n\n\n套接字程序类型\n应用场景\n挂载方法\n\n\n\nBPF_PROG_TYPE_SOCKET_FILTER\n用于套接字过滤和观测\n用户态程序可通过系统调用 setsockopt(sock, SOL_SOCKET, SO_ATTACH_BPF,…），绑定 BPF 程序到具体的 socket 上\n\n\nBPF_PROG_TYPE_SOCKOPS\n用于套接字修改或重定向\n用户态程序可通过 BPF 系统调用的 BPF_PROG_ATTACH 命令（指定挂载类型为 BPF_CGROUP_SOCK_OPS)，将其挂载到 cgroup 上\n\n\nBPF_PROG_TYPE_SKSKB\n用于套接字修改或消息流动态解析\n用户态程序可通过 BPF 系统调用的 BPF_PROG_ATTACH 命令（指定挂载类型为 BPF_SK_SKB_STREAM_VERDICT 或 BPF_SK_SKB_STREAM_PARSER)，将其挂载到 BPF_MAP_TYPE_SOCKMAP 类型的 BPF 映射上\n\n\nBPF_PROG_TYPESKMSG\n用于控制内核是否发送消息到套接字\n用户态程序可通过 BPF 系统调用的 BPF_PROG_ATTACH 命令（指定挂载类型为 BPF_SK_MSG_VERDICT) 将其挂载到 BPF_MAP_TYPE_SOCKMAP 类型的 BPF 映射上\n\n\nBPF_PROG_TYPE_SK_REUSEPORT\n用于控制端口是否重用\n用户态程序可通过系统调用 setsockopt(sock, SOL_SOCKET, SO_ATTACH_REUSEPORT_EBPF, …），绑定 BPF 程序到具体的 socket 上\n\n\nBPF_PROG_TYPE_SK_LOOKUP\n用于为新的 TCP 连接选择监听套接字，或为 UDP 数据包选择未连接的套接字，可用来绕过 bind 系统调用的限制\n用户态程序可通过系统调用 bpf(BPF_LINK_CREATE,.，绑定 BPF 程序到网 络命名空间 (netns)上\n\n\ncgroup用于对 cgroup 内所有进程的网络过滤、套接字选项以及转发等进行动态控制，最典型的应用场景是对容器中运行的多个进程进行网络控制:\n\n\n\ncgroup 程序类型\n应用场景\n\n\n\nBPF_PROG_TYPECGROUP_SKB\n在入口和出口过滤数据包，并可以接受或拒绝数据包\n\n\nBPF_PROG_TYPE_CGROUP_SOCK\n在套接字创建、释放和绑定地址时，接受或拒绝操作，也可用来统计套接字信息\n\n\nBPF_PROG_TYPE_CGROUP_SOCKOPT\n在 setsockopt 和 getsockopt 操作中修改套接字选项\n\n\nBPF_PROG_TYPE_CGROUP_SOCKADDR\n在 connect、 bind、 sendto 和 recvmsg 操作中，修改 IP 地址和端口\n\n\nBPF_PROG_TYPE_CGROUP_DEVICE\n对设备文件的访问进行过滤\n\n\nBPF_PROG_TYPE_CGROUP_SYSCTL\n对 sysctl 的访问进行过滤\n\n\n其他类型\n\n\nBPF程序类型\n应用场景\n\n\n\nBPF PROG_TYPE_LSM\n用于 Linux 安全模块(Linux Security Module,LSM）访问控制和审计策略\n\n\nBPF_PROG_TYPE_LWT_INBPF_PROG_TYPE_LWT_OUTBPF_PROG_TYPE_LWT_XMIT\n用于轻量级隧道（如 vxlan、 mpls 等）的封装或解封装\n\n\nBPF_PROG_TYPE_LIRC_MODE2\n用于红外设备的远程遥控\n\n\nBPF_PROG_TYPE_STRUCT_OPS\n用于修改内核结构体，目前仅支持拥塞控制算法 tcp_congestion_ops\n\n\nBPF_PROG_TYPE_FLOW_DISSECTOR\n用于内核流量解析器 (Flow Dissector)\n\n\nBPF_PROG_TYPE_EXT\n用于扩展BPF程序\n\n\nbpftracebpftrace 在 eBPF 和 BCC 之上构建了一个简化的跟踪语言，通过简单的几行脚本，就可以实现复杂的追踪功能。如下图 所示, bpftrace 会把开发的脚本借助 BCC 编译加载到内核中执行，再通过 BPF 映射获取执行结果:\n\n通过 bpftrace -l 可以查询内核插桩和跟踪点 -v 可以查看函数的入口参数和返回值。\n\n在内核插桩和跟踪点两者都可以用的情况下，应该选择更稳定的跟踪点，以保证 eBPF 程序的可移植性(即在不同的内核中都可以正常执行)。\n\nroot@f287ed60a8b5:/# bpftrace -lv &#x27;tracepoint:syscalls:sys_enter_execve&#x27;tracepoint:syscalls:sys_enter_execve    int __syscall_nr    const char __attribute__((user)) * filename    const char __attribute__((user)) *const __attribute__((user)) * argv    const char __attribute__((user)) *const __attribute__((user)) * envproot@f287ed60a8b5:/# bpftrace -lv &#x27;tracepoint:syscalls:sys_exit_execve&#x27;tracepoint:syscalls:sys_exit_execve    int __syscall_nr    long retroot@f287ed60a8b5:/# bpftrace -lv &#x27;tracepoint:syscalls:sys_enter_execveat&#x27;tracepoint:syscalls:sys_enter_execveat    int __syscall_nr    int fd    const char __attribute__((user)) * filename    const char __attribute__((user)) *const __attribute__((user)) * argv    const char __attribute__((user)) *const __attribute__((user)) * envp    int flagsroot@f287ed60a8b5:/# bpftrace -lv &#x27;tracepoint:syscalls:sys_exit_execveat&#x27;tracepoint:syscalls:sys_exit_execveat    int __syscall_nr    long ret\n\n通过这个代码可以达到上面的 BCC 事例的效果:\nroot@f287ed60a8b5:~/code/session7/bpftrace# bpftrace sys_enter.btAttaching 2 probes...14:37:54  1785   0      bash            bpftool prog list\n\n查询用户程序跟踪点可以通过 readelf 查询二进制文件的基本信息，使用这个 Go 语言项目来示范:\npackage mainimport (\t&quot;fmt&quot;\t&quot;math/rand&quot;\t&quot;time&quot;)func main() &#123;\trand.Seed(int64(time.Now().Nanosecond()))\ti := rand.Intn(10)\ta := callbackTP(i)\tfmt.Println(a)&#125;// go:noinlinefunc callbackTP(i int) int &#123;\tif i &lt; 5 &#123;\t\treturn i\t&#125;\tfor a := 0; a &lt; i; a++ &#123;\t\tfmt.Printf(&quot;xxxxx %d&quot;, a)\t&#125;\treturn 99&#125;\n\n可以在上面的容器环境中调试，cd /root/code/session9 &amp;&amp; go build main.go 进行编译，然后通过 readelf 查看对应的信息:\n# 查询符号表root@76f664f7d01b:~/code/session9# readelf -Ws ./mainSymbol table &#x27;.symtab&#x27; contains 2136 entries:   Num:    Value          Size Type    Bind   Vis      Ndx Name     0: 0000000000000000     0 NOTYPE  LOCAL  DEFAULT  UND     1: 0000000000000000     0 FILE    LOCAL  DEFAULT  ABS go.go     2: 0000000000401000     0 FUNC    LOCAL  DEFAULT    1 runtime.text     3: 0000000000402140   557 FUNC    LOCAL  DEFAULT    1 cmpbody     4: 00000000004023a0   318 FUNC    LOCAL  DEFAULT    1 memeqbody     5: 0000000000402520   279 FUNC    LOCAL  DEFAULT    1 indexbytebody     6: 0000000000459f60    64 FUNC    LOCAL  DEFAULT    1 gogo     7: 0000000000459fa0    53 FUNC    LOCAL  DEFAULT    1 callRet     8: 0000000000459fe0    47 FUNC    LOCAL  DEFAULT    1 gosave_systemstack_switch     9: 000000000045a020    13 FUNC    LOCAL  DEFAULT    1 setg_gcc    10: 000000000045a040  1370 FUNC    LOCAL  DEFAULT    1 aeshashbody    11: 000000000045a5a0    75 FUNC    LOCAL  DEFAULT    1 debugCall32......# 查询USDT信息（USDT信息位于ELF文件的notes段）root@76f664f7d01b:~/code/session9# readelf -n ./mainDisplaying notes found in: .note.go.buildid  Owner                Data size \tDescription  Go                   0x00000053\tGO BUILDID   description data: 4e 6d 53 72 46 41 6d 53 44 30 6a 69 5a 6e 73 51 55 5a 74 55 2f 45 56 77 62 5a 5a 33 31 33 35 59 4f 76 5f 51 75 30 70 5a 61 2f 43 46 50 62 66 2d 67 78 4a 39 74 31 61 69 78 53 51 4b 57 32 2f 33 76 67 6b 73 51 37 38 5a 4f 73 7a 4c 5a 33 6e 41 4d 4d 52\n\n也可以通过 bpftrace 来查看:\n# 查询 uproberoot@76f664f7d01b:~/code/session9# bpftrace -l &quot;uprobe:./main:*&quot;uprobe:./main:_rt0_amd64uprobe:./main:_rt0_amd64_linuxuprobe:./main:aeshashbodyuprobe:./main:callRetuprobe:./main:cmpbodyuprobe:./main:debugCall1024uprobe:./main:debugCall128uprobe:./main:debugCall16384uprobe:./main:debugCall2048uprobe:./main:debugCall256uprobe:./main:debugCall32uprobe:./main:debugCall32768uprobe:./main:debugCall4096# 查询 usdt，这里的 Go 项目没有，通过 /usr/lib/x86_64-linux-gnu/libc.so.6 来替换root@76f664f7d01b:~/code/session9# bpftrace -l &#x27;usdt:/usr/lib/x86_64-linux-gnu/libc.so.6:*&#x27;usdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:cond_broadcastusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:cond_destroyusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:cond_initusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:cond_signalusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:cond_waitusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:lll_lock_waitusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:lll_lock_wait_privateusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:longjmpusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:longjmp_targetusdt:/usr/lib/x86_64-linux-gnu/libc.so.6:libc:memory_arena_new\n\n\n想要通过二进制文件查询符号表和参数定义，必须在编译的时候保留 DWARF 调试信息。\n\n跟踪 Go 程序的执行Terminal1:root@76f664f7d01b:~/code/session9# bpftrace -e &#x27;        uprobe:./main:main.callbackTP &#123; printf(&quot;main.callbackTP i: %d =&gt; &quot;, reg(&quot;ax&quot;)) &#125;        uretprobe:./main:main.callbackTP &#123; printf(&quot;return %d\\n&quot;, retval) &#125;&#x27;Attaching 2 probes...main.callbackTP i: 0 =&gt; return 0main.callbackTP i: 2 =&gt; return 2main.callbackTP i: 8 =&gt; return 99main.callbackTP i: 6 =&gt; return 99\n\nTerminal2# champly @ champlydeiMac in ~ [10:55:20]$ docker exec -it ebpf-for-mac bashroot@76f664f7d01b:/# cd /root/code/session9root@76f664f7d01b:~/code/session9# ./main0root@76f664f7d01b:~/code/session9# ./main2root@76f664f7d01b:~/code/session9# ./mainxxxxx 0xxxxx 1xxxxx 2xxxxx 3xxxxx 4xxxxx 5xxxxx 6xxxxx 799root@76f664f7d01b:~/code/session9# ./mainxxxxx 0xxxxx 1xxxxx 2xxxxx 3xxxxx 4xxxxx 599\n\n可以看到可以查询到 callbackTP 函数调用的入参和返回值。\nlibbpf通过这个 libbpf 代码 可以实现对应 BCC 代码 相同的功能，通过 libbpf 代码编译的结果不能在上面提到的容器环境中运行，可以把编译的二进制文件拷贝到对应的虚拟机中直接运行。\n这个项目也可以使用 cilium 的 ebpf 库，通过 Go 语言进行开发用户态程序，这个是可以直接在容器环境中编译的。\n区别\nbpftrace 通常用在快速排查和定位系统上，它支持用单行脚本的方式来快速开发并执行一个 eBPF 程序。不过功能有限，不支持特别复杂的 eBPF 程序;\nBCC 通常在开发复杂的 eBPF 程序中，其内置的各种小工具也是目前应用最为广泛的 eBPF 小程序。不过需要依赖 LLVM 和内核头文件才可以动态编译和加载 eBPF 程序;\nlibbpf 是从内核中抽离出来的标准库，用它开发的 eBPF 程序可以直接分发执行，这样就不需要每台机器都安装 LLVM 和内核头文件了。不过要求内核开启 BTF 特性。\n\n应用网络跟踪如果不知道怎么跟踪哪个函数调用，可以把所有的跟踪点都打印出来，下面的事例需要在虚拟机中实验，要不然打印不了调用栈:\nchamply@ubuntu:~$ sudo bpftrace -e &#x27;tracepoint:net:* &#123; printf(&quot;%s(%d): %s %s\\n&quot;, comm, pid, probe, kstack()); &#125;&#x27;Attaching 18 probes...swapper/5(0): tracepoint:net:napi_gro_receive_entry        napi_gro_receive+207        napi_gro_receive+207        e1000_clean_rx_irq+443        e1000_clean+621        __napi_poll+49        net_rx_action+575        __softirqentry_text_start+207        irq_exit_rcu+164        common_interrupt+138        asm_common_interrupt+30        native_safe_halt+14        acpi_idle_enter+91        cpuidle_enter_state+141        cpuidle_enter+46        call_cpuidle+35        do_idle+486        cpu_startup_entry+32        start_secondary+287        secondary_startup_64_no_verify+194\n\n如果需要排查丢包的问题，可以查询内核 SKB 文档，可以看到内核释放 SKB 有两个地方:\n\nkfree_skb：经常在网络异常丢包时调用;\nconsume_skb：在正常网络连接完成的时候调用。\n\n以下是访问 google.com 获取到的结果:\nchamply@ubuntu:~$ sudo bpftrace -e &#x27;kprobe:kfree_skb /comm==&quot;curl&quot;/ &#123;printf(&quot;kstack: %s\\n&quot;, kstack);&#125;&#x27;Attaching 1 probe...kstack:        kfree_skb+1        udpv6_destroy_sock+57        sk_common_release+34        udp_lib_close+9        inet_release+75        inet6_release+49        __sock_release+66        sock_close+21        __fput+159        ____fput+14        task_work_run+112        exit_to_user_mode_prepare+437        syscall_exit_to_user_mode+39        do_syscall_64+110        entry_SYSCALL_64_after_hwframe+68kstack:        kfree_skb+1        udpv6_destroy_sock+57        sk_common_release+34        udp_lib_close+9        inet_release+75        inet6_release+49        __sock_release+66        sock_close+21        __fput+159        ____fput+14        task_work_run+112        exit_to_user_mode_prepare+437        syscall_exit_to_user_mode+39        do_syscall_64+110        entry_SYSCALL_64_after_hwframe+68kstack:        kfree_skb+1        unix_release+29        __sock_release+66        sock_close+21        __fput+159        ____fput+14        task_work_run+112        exit_to_user_mode_prepare+437        syscall_exit_to_user_mode+39        do_syscall_64+110        entry_SYSCALL_64_after_hwframe+68kstack:        kfree_skb+1        __sys_connect_file+95        __sys_connect+160        __x64_sys_connect+26        do_syscall_64+97        entry_SYSCALL_64_after_hwframe+68kstack:        kfree_skb+1        __sys_connect_file+95        __sys_connect+160        __x64_sys_connect+26        do_syscall_64+97        entry_SYSCALL_64_after_hwframe+68\n\n负载均衡通过虚拟机环境的不同 Docker 容器来完成相关的步骤，具体的架构如下所示:\nflowchart\n\nclient[Client: wrk] --&gt; LB[LB:172.17.0.4]\nLB --&gt; http1[HTTP1:172.17.0.2]\nLB --&gt; http2[HTTP2:172.17.0.3]\n\n套接字优化eBPF 程序: sockops.bpf.c、sockredir.bpf.c、sockops.h\n具体实验步骤:\nchamply@ubuntu:~/eBPF/example/session12$ docker run -itd --rm --name=http1 --hostname=http1 feisky/webserver33fb44488527c70424ff55793fdfc34d9004cde1e177b85a6e480f65dff04f2echamply@ubuntu:~/eBPF/example/session12$ docker run -itd --rm --name=http2 --hostname=http2 feisky/webserverd6428353fd79c21446923f7c50f74dc35e9f9b6de60adefe372d9004f490e9b3champly@ubuntu:~/eBPF/example/session12$ docker run -itd --rm --name=nginx nginxed5f0acc0f9b74f5767164ef97c9826eca1586049230a2a8d50179ec6e492702champly@ubuntu:~/eBPF/example/session12$ IP1=$(docker inspect http1 -f &#x27;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27;)champly@ubuntu:~/eBPF/example/session12$ IP2=$(docker inspect http2 -f &#x27;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27;)champly@ubuntu:~/eBPF/example/session12$ echo &quot;Webserver1&#x27;s IP: $IP1&quot;Webserver1&#x27;s IP: 172.17.0.2champly@ubuntu:~/eBPF/example/session12$ echo &quot;Webserver2&#x27;s IP: $IP2&quot;Webserver2&#x27;s IP: 172.17.0.3champly@ubuntu:~/eBPF/example/session12$ cat&gt;nginx.conf &lt;&lt;EOF&gt; user  nginx;&gt; worker_processes  auto;&gt;&gt; error_log  /var/log/nginx/error.log notice;&gt; pid        /var/run/nginx.pid;&gt;&gt; events &#123;&gt;     worker_connections  1024;&gt; &#125;&gt;&gt; http &#123;&gt;    include       /etc/nginx/mime.types;&gt;    default_type  application/octet-stream;&gt;&gt;     upstream webservers &#123;&gt;         server $IP1;&gt;         server $IP2;&gt;     &#125;&gt;&gt;     server &#123;&gt;         listen 80;&gt;&gt;         location / &#123;&gt;             proxy_pass http://webservers;&gt;         &#125;&gt;     &#125;&gt; &#125;&gt; EOFchamply@ubuntu:~/eBPF/example/session12$ docker cp nginx.conf nginx:/etc/nginx/nginx.confchamply@ubuntu:~/eBPF/example/session12$ docker exec nginx nginx -s reload2022/12/05 09:04:56 [notice] 37#37: signal process startedchamply@ubuntu:~/eBPF/example/session12$ IP_LB=$(docker inspect nginx -f &#x27;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&#x27;)champly@ubuntu:~/eBPF/example/session12$ docker run -it --rm --name=client skandyla/wrk -c100 http://$IP_LBRunning 10s test @ http://172.17.0.4  2 threads and 100 connections  Thread Stats   Avg      Stdev     Max   +/- Stdev    Latency     8.26ms    6.08ms  60.22ms   72.11%    Req/Sec     6.53k     0.92k    8.69k    68.00%  130064 requests in 10.02s, 20.47MB readRequests/sec:  12979.12Transfer/sec:      2.04MBchamply@ubuntu:~/eBPF/example/session12$ sudo bpftool prog load sockops.bpf.o /sys/fs/bpf/sockops type sockops pinmaps /sys/fs/bpfchamply@ubuntu:~/eBPF/example/session12$ sudo bpftool prog load sockredir.bpf.o /sys/fs/bpf/sockredir type sk_msg map name sock_ops_map pinned /sys/fs/bpf/sock_ops_mapchamply@ubuntu:~/eBPF/example/session12$ sudo bpftool cgroup attach /sys/fs/cgroup/unified sock_ops pinned /sys/fs/bpf/sockopschamply@ubuntu:~/eBPF/example/session12$ sudo bpftool prog attach pinned /sys/fs/bpf/sockredir msg_verdict pinned /sys/fs/bpf/sock_ops_mapchamply@ubuntu:~/eBPF/example/session12$ docker run -it --rm --name=client skandyla/wrk -c100 http://$IP_LBRunning 10s test @ http://172.17.0.4  2 threads and 100 connections  Thread Stats   Avg      Stdev     Max   +/- Stdev    Latency     7.84ms    5.76ms  54.50ms   71.07%    Req/Sec     6.89k     0.87k    8.97k    63.50%  137069 requests in 10.02s, 21.57MB readRequests/sec:  13676.41Transfer/sec:      2.15MBchamply@ubuntu:~/eBPF/example/session12$ sudo bpftool prog detach pinned /sys/fs/bpf/sockredir msg_verdict pinned /sys/fs/bpf/sock_ops_mapchamply@ubuntu:~/eBPF/example/session12$ sudo bpftool cgroup detach /sys/fs/cgroup/unified sock_ops name bpf_sockmapchamply@ubuntu:~/eBPF/example/session12$ sudo rm -rf /sys/fs/bpf/sockops /sys/fs/bpf/sockredir /sys/fs/bpf/sock_ops_mapchamply@ubuntu:~/eBPF/example/session12$ docker run -it --rm --name=client skandyla/wrk -c100 http://$IP_LBRunning 10s test @ http://172.17.0.4  2 threads and 100 connections  Thread Stats   Avg      Stdev     Max   +/- Stdev    Latency     8.29ms    6.15ms  50.12ms   70.69%    Req/Sec     6.52k     0.93k    8.50k    64.00%  129810 requests in 10.03s, 20.43MB readRequests/sec:  12948.24Transfer/sec:      2.04MBchamply@ubuntu:~/eBPF/example/session12$\n\nXDP优化就不要 LB 里面的 Nginx 服务，直接使用 XDP 程序完成转发，具体的实验代码在 session13。\ncontainer:\tdocker run -itd --name=lb --privileged -v /sys/kernel/debug:/sys/kernel/debug alpinebuild:\tclang -g -O2 -target bpf -D__TARGET_ARCH_x86 -I/usr/include/x86_64-linux-gnu -I. -c xdp-proxy.bpf.c -o xdp-proxy.bpf.o\tbpftool gen skeleton xdp-proxy.bpf.o &gt; xdp-proxy.skel.h\tclang -g -O2 -Wall -I. -c xdp-proxy.c -o xdp-proxy.o\tclang -Wall -O2 -g xdp-proxy.o -static -lbpf -lelf -lz -o xdp-proxyrun:\t# 复制字节码到容器中\tdocker cp xdp-proxy.bpf.o lb:/\t# 在容器中安装iproute2命令行工具\tdocker exec -it lb apk add iproute2 --update\t# 在容器中挂载XDP程序到eth0网卡\tdocker exec -it lb ip link set dev eth0 xdpgeneric object xdp-proxy.bpf.o sec xdp\n\n\n参考链接:\n\nhttps://ebpf.io/what-is-ebpf/\nhttps://www.brendangregg.com/ebpf.html\nhttps://time.geekbang.org/column/intro/100104501\nhttps://github.com/champly/ebpf-learn-code\nhttps://www.ebpf.top/post/ebpf_learn_path&#x2F;\nhttps://github.com/iovisor/bcc\nhttps://github.com/libbpf/libbpf\nhttps://github.com/cilium/ebpf\nhttps://www.usenix.org/conference/lisa21/presentation/gregg-bpf\nhttps://docs.cilium.io/en/stable/bpf/\nhttps://github.com/iovisor/bpftrace\nhttps://www.kernel.org/doc/htmldocs/networking/ch01s02.html\n\n","categories":["eBPF"],"tags":["eBPF"]},{"title":"Kubernetes cni 网络插件调试","url":"/2019/07/29/network/kubernetes-cni-debug/","content":"最近搭建 Kubernetes 集群的时候使用的网络插件是 bridge + host-local\n关于cni插件安装 kubelet 的时候会有一个 kubernetes-cni-version-0.x86_64.rpm 的依赖文件，安装了之后会在 /opt/cni/bin 下面会有各种网络插件\n# rpm -qpl kubernetes-cni-0.7.5-0.x86_64.rpmwarning: kubernetes-cni-0.7.5-0.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 3e1ba8d5: NOKEY/opt/cni/opt/cni/bin/opt/cni/bin/bridge/opt/cni/bin/dhcp/opt/cni/bin/flannel/opt/cni/bin/host-device/opt/cni/bin/host-local/opt/cni/bin/ipvlan/opt/cni/bin/loopback/opt/cni/bin/macvlan/opt/cni/bin/portmap/opt/cni/bin/ptp/opt/cni/bin/sample/opt/cni/bin/tuning/opt/cni/bin/vlan\n\n所有的 cni 插件在 spec-v0.3.1 之前只实现两个接口 add, del。在 spec-v0.4.0 之后会在 del 之前执行 check ,所以多了一个 check 接口。\n版本差异: Container Network Interface Specification\n配置文件使用的 cni 版本cni 插件使用的插件配置地址 /etc/cni/net.d/ 下面的文件，根据排序取第一个配置文件信息\n# cat /etc/cni/net.d/cni.conf&#123;    &quot;cniVersion&quot;: &quot;0.3.1&quot;,    &quot;name&quot;: &quot;mynet&quot;,    &quot;type&quot;: &quot;bridge&quot;,    &quot;bridge&quot;: &quot;cni0&quot;,    &quot;isDefaultGateway&quot;: true,    &quot;forceAddress&quot;: false,    &quot;ipMasq&quot;: true,    &quot;hairpinMode&quot;: true,    &quot;ipam&quot;: &#123;        &quot;type&quot;: &quot;host-local&quot;,        &quot;ranges&quot;: [            [                &#123;                    &quot;subnet&quot;: &quot;10.13.0.0/22&quot;,                    &quot;rangeStart&quot;: &quot;10.13.3.8&quot;,                    &quot;rangeEnd&quot;: &quot;10.13.3.253&quot;,                    &quot;gateway&quot;: &quot;10.13.3.254&quot;                &#125;            ]        ],        &quot;routes&quot;: [            &#123;                &quot;dst&quot;: &quot;0.0.0.0/0&quot;            &#125;        ],        &quot;dataDir&quot;: &quot;/opt/data/cni&quot;    &#125;&#125;\n\n配置具体信息可以查看 源码plugins里面的插件README.md\n比如我们使用的 host-local，我需要知道 cni 版本怎么查看呢？\n查看安装的 kubernetes-cni 版本安装的时候知道是 kubernetes-cni-0.7.5-0.x86_64.rpm，所以对应的版本信息是 0.7.5\n查看源码 host-local 注册的版本信息选择 plugin 插件版本是 0.7.5，查看 host-local 注册信息 源码\nfunc main() &#123;\tskel.PluginMain(cmdAdd, cmdDel, version.All)&#125;\n\n可以看到版本是All\n查看 plugin 使用的 cni 版本同上一步，选择源码文件的提交 tag 为 0.7.5，查看 plugins 使用的 cni 版本信息 源码\n// Legacy PluginInfo describes a plugin that is backwards compatible with the// CNI spec version 0.1.0.  In particular, a runtime compiled against the 0.1.0// library ought to work correctly with a plugin that reports support for// Legacy versions.//// Any future CNI spec versions which meet this definition should be added to// this list.var Legacy = PluginSupports(&quot;0.1.0&quot;, &quot;0.2.0&quot;)var All = PluginSupports(&quot;0.1.0&quot;, &quot;0.2.0&quot;, &quot;0.3.0&quot;, &quot;0.3.1&quot;)\n\n可以看到All对应的版本信息支持 “0.1.0”, “0.2.0”, “0.3.0”, “0.3.1”,所以我就可以写 0.3.1 了\n调试 cni在上面我们的 cni 版本是 0.3.1,所以在查看 源码 的时候选择 spec-v0.3.1\n在 README.md 里面有测试方法\n$ CNI_PATH=$GOPATH/src/github.com/containernetworking/plugins/bin$ cd $GOPATH/src/github.com/containernetworking/cni/scripts$ sudo CNI_PATH=$CNI_PATH ./priv-net-run.sh ifconfigeth0      Link encap:Ethernet  HWaddr f2:c2:6f:54:b8:2b            inet addr:10.22.0.2  Bcast:0.0.0.0  Mask:255.255.0.0          inet6 addr: fe80::f0c2:6fff:fe54:b82b/64 Scope:Link          UP BROADCAST MULTICAST  MTU:1500  Metric:1          RX packets:1 errors:0 dropped:0 overruns:0 frame:0          TX packets:0 errors:0 dropped:1 overruns:0 carrier:0          collisions:0 txqueuelen:0          RX bytes:90 (90.0 B)  TX bytes:0 (0.0 B)lo        Link encap:Local Loopback            inet addr:127.0.0.1  Mask:255.0.0.0          inet6 addr: ::1/128 Scope:Host          UP LOOPBACK RUNNING  MTU:65536  Metric:1          RX packets:0 errors:0 dropped:0 overruns:0 frame:0          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0          collisions:0 txqueuelen:0          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n\n其中我们只需要使用到 scripts 下面的 priv-net-run.sh 和 exec-plugins.sh 这两个文件\n把这两个文件下载到本地，然后添加可执行权限\n添加 /etc/cni/net.d/ 下面的 cni 配置，添加 CNI_PATH\n[root@10 tmp]# export CNI_PATH=/opt/cni/bin/[root@10 tmp]# ./priv-net-run.sh\n\n其中 /opt/cni/bin 就是 kubernetes-cni-0.7.5-0.x86_64.rpm 对应的插件目录，如果没有修改就是用这个就可以了\n可以修改 shell 脚本来调试 cni 插件，比如我下面修改之后可以看到执行过程\n[root@10 tmp]# ./priv-net-run.shadd 7ac145c133dc63c2 /var/run/netns/7ac145c133dc63c2netconf:/etc/cni/net.d/cni.confname:mynetplugin:bridgeres:&#123;    &quot;cniVersion&quot;: &quot;0.3.1&quot;,    &quot;interfaces&quot;: [        &#123;            &quot;name&quot;: &quot;cni0&quot;,            &quot;mac&quot;: &quot;76:02:71:5b:9c:79&quot;        &#125;,        &#123;            &quot;name&quot;: &quot;vetheb50e2bb&quot;,            &quot;mac&quot;: &quot;76:02:71:5b:9c:79&quot;        &#125;,        &#123;            &quot;name&quot;: &quot;eth0&quot;,            &quot;mac&quot;: &quot;ce:bf:1f:fc:ff:d1&quot;,            &quot;sandbox&quot;: &quot;/var/run/netns/7ac145c133dc63c2&quot;        &#125;    ],    &quot;ips&quot;: [        &#123;            &quot;version&quot;: &quot;4&quot;,            &quot;interface&quot;: 2,            &quot;address&quot;: &quot;10.13.3.23/22&quot;,            &quot;gateway&quot;: &quot;10.13.3.254&quot;        &#125;    ],    &quot;routes&quot;: [        &#123;            &quot;dst&quot;: &quot;0.0.0.0/0&quot;        &#125;,        &#123;            &quot;dst&quot;: &quot;0.0.0.0/0&quot;,            &quot;gw&quot;: &quot;10.13.3.254&quot;        &#125;    ],    &quot;dns&quot;: &#123;&#125;&#125;No command specifieddel 7ac145c133dc63c2 /var/run/netns/7ac145c133dc63c2netconf:/etc/cni/net.d/cni.confname:mynetplugin:bridgeres:\n\n\n参考资料:\n\n浅谈k8s cni 插件\n\n","categories":["CloudNative"],"tags":["Kubernetes","CNI"]},{"title":"Kubernetes 网络问题排查","url":"/2019/08/21/network/kubernetes-network-problem/","content":"安全组当一个网络包进入网卡的时候，首先拿下 mac 头看看是不是当前网卡的。\n\n如果是，则拿下 IP 头，得到了 IP 之后，就开始进行路由判断。在路由判断之前这个节点称为 PREROUTING。\n如果发现ip是当前网卡 IP，包就应该发给上面的传输层，这个节点叫做 INPUT。\n如果发现 IP 不是当前网卡的，就需要进行转发，这个节点就叫 FORWARD。\n如果ip是当前网卡的 IP，则发送到上层处理。处理完一般会返回一个结果，把处理结果发出去，这个节点称为 OUTPUT。\n无论是 FORWARD 和 OUTPUT，都是在路由判断之后发生的，最有一个节点是 POSTROUTING。\n\niptables 模块在 Linux 内核中，有一个框架叫 Netfilter。可以在上面的几个节点放一个hook函数，这些函数可以对数据包进行干预。如果接受就是 ACCEPT;如果需要过滤掉就是 DROP;如果需要发送给用户态进程处理，就是 QUEUE。\niptables 就是实现了 Netfilter 框架，在上面五个节点上都放了hook函数，按照功能可以分为:\n\nconntrack: 连接跟踪\nfilter: 数据包过滤\nnat: 网络地址转换\nmangle: 数据包修改\n\niptables在用户态，有一个客户端程序 iptables,用命令行来干预内核的规则,内核的功能对于 iptables 来说，就是表和链的概念。\n表\nraw\nmangle\nnat\nfilter\n\n优先级: raw &gt; mangle &gt; nat &gt; filter\nraw 不常用，主要功能都在剩下的几个表里面\nfilter 表处理过滤功能\n\nINPUT 链: 过滤所有目标地址是本机的数据包\nFORWARD 链: 过滤所有路过本机的数据包\nOUTPUT 链: 过滤所有由本机产生的数据包\n\nnat 表\nPREROUTING 链: 可以在数据包到达防火墙之前改变目标地址(DNT)\nOUTPUT 链: 可以改变本地产生的数据包的目标地址\nPOSTROUTING 链: 在数据包离开防火墙时改变数据包的源地址(SNAT)\n\nmangle 表\nPREROUTING 链\nINPUT 链\nFORWARD 链\nOUTPUT 链\nPOSTROUTING 链\n\nraw 表\nPREROUTING 链\nOUTPUT 链\n\n将 iptables 的表和链整合起来就形成了下面的图和过程。\n\nKubernetes 里的 Service** Service 是由 kube-proxy 组件加上 iptables 来共同实现的**\niptables 模式当我们创建的 service 提交到 Kubernetes 的时候，kube-proxy 就可以通过 Service 的 Informer 感知到 Service 对象添加。从而对这个事件进行响应，它会在宿主机上创建一条 iptables 规则。可以使用 iptables-save 看见这样一条规则:\n-A KUBE-SERVICES -d 10.27.248.11/32 -p tcp -m comment --comment &quot;default/hostnames: cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3\n\n凡是目的地址是 10.27.248.11,端口号是80的包,都使用 KUBE-SVC-NWV5X2332I4OT4T3 iptables 链处理，而这个 10.27.248.11 这个就是 Service 的 clusterIP，在查看 KUBE-SVC-NWV5X2332I4OT4T3 规则，实际上是一组规则集合:\n-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment &quot;default/hostnames:&quot; -j KUBE-SEP-57KPRZ3JQVENLNBR\n\n这实际上是一组随机模式(–mode random)的 iptables 链,而 KUBE-SEP-(hash) 链指向的最终地址就是代理的三个 Pod(就是 Endpoint)。所以这一组规则就是 Service 实现负载均衡的位置。查看上述三条链明细,就能理解 Service 转发的具体原理：\n-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.28.1.123/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.28.1.123:8080-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.28.1.22/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.28.1.22:8080-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.28.3.11/32 -m comment --comment &quot;default/hostnames:&quot; -j MARK --set-xmark 0x00004000/0x00004000-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment &quot;default/hostnames:&quot; -m tcp -j DNAT --to-destination 10.28.3.11:8080\n\n这三条链其实就是三条 DNAT 规则。iptables 对流入的 IP 包还设置了一个标志(--set-xmark)，在 PREROUTING 检查之前将流入 IP 包的目的地址和端口改成 --to-destination 所指定的新的地址和端口\nIPVS 模式基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级 Pod 的主要障碍，IPVS 并不需要在主机上为每个 Pod 设置 iptables 规则，而是把这些规则放到了内核态，从而极大地降低了维护这些规则的代价。IPVS 模式的工作原理，和 iptables 类似。当创建了 Service 后，kube-proxy 首先会在宿主机上创建一个虚拟网卡 kube-ipvs0。并为他分配 Service VIP 作为 IP 地址:\n# ip addr  ...  73：kube-ipvs0：&lt;BROADCAST,NOARP&gt;  mtu 1500 qdisc noop state DOWN qlen 1000  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff  inet 10.27.248.11/32  scope global kube-ipvs0  valid_lft forever  preferred_lft forever\n\n然后 kube-proxy 就会通过 Linux 的 IPVS 模块，为这个IP地址设置三个 IPVS 虚拟主机，可以通过 ipvsadm 查看:\n# ipvsadm -ln IP Virtual Server version 1.2.1 (size=4096)  Prot LocalAddress:Port Scheduler Flags    -&gt;  RemoteAddress:Port           Forward  Weight ActiveConn InActConn       TCP  10.27.248.11:80 rr    -&gt;  10.28.1.123:8080   Masq    1       0          0             -&gt;  10.28.1.22:8080    Masq    1       0          0    -&gt;  10.28.3.11:8080    Masq    1       0          0\n\n这三个 IPVS 虚拟主机的 IP 地址和端口对应的就是被代理的 Pod，之间使用轮询模式(rr)来作为负载均衡。\nIPVS 模块只负责负载均衡和代理功能，而一个完整的 Service 流程正常工作需要的包过滤，SNAT，DNAT 等操作，还是要靠 iptables 来实现。只不过这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量增加而增加\n查看 iptables nat 表\n-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES-A OUTPUT -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES-A POSTROUTING -m comment --comment &quot;kubernetes postrouting rules&quot; -j KUBE-POSTROUTING-A KUBE-FIREWALL -j KUBE-MARK-DROP-A KUBE-LOAD-BALANCER -j KUBE-MARK-MASQ-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000-A KUBE-NODE-PORT -p tcp -m comment --comment &quot;Kubernetes nodeport TCP port for masquerade purpose&quot; -m set --match-set KUBE-NODE-PORT-TCP dst -j KUBE-MARK-MASQ-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000/0x4000 -j MASQUERADE-A KUBE-POSTROUTING -m comment --comment &quot;Kubernetes endpoints dst ip:port, source ip for solving hairpin purpose&quot; -m set --match-set KUBE-LOOP-BACK dst,dst,src -j MASQUERADE-A KUBE-SERVICES ! -s 10.27.248.0/22 -m comment --comment &quot;Kubernetes service cluster ip + port for masquerade purpose&quot; -m set --match-set KUBE-CLUSTER-IP dst,dst -j KUBE-MARK-MASQ-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODE-PORT-A KUBE-SERVICES -m set --match-set KUBE-CLUSTER-IP dst,dst -j ACCEPT\n\n倒数第 3 条规则表示来源 IP 不是 10.27.248.0/22，则进入 KUBE-MARK-MASQ，打上 0x4000 标。倒数第 5 条规则就是在 POSTROUTING 的时候做 SNAT,如果看到有 0x4000 这个标志的，就修改来源 IP(MASQUERADE)。\nSNAT 需要指定一个或多个 IP，MASQUERADE 不需要指定，直接获取网卡的 IP 作为来源 IP\n问题和解决思路Service 没办法通过 DNS 访问区分到底是 Service 本身的配置问题还是集群的 DNS 除了问题，检查的有效方式就是 Kubernetes 自己的 master 节点的 Service DNS 是否正常:\n# 在pod里面执行nslookup kubernetes.default\n\n如果执行有问题，那么就应该检查 kube-dns 的运行状态和日志。否则的话就应该检查 Service 定义是不是有问题。\nService 没办法通过 ClusterIP 访问首先查看时候有 Endpoint(kubectl get endpoint xxxx)，如果 Pod 的 readniessProbe 没有通过,也不会出现在 Endpoint 列表里面。如果 Endpoint 正常，就需要确认一下 kube-proxy 是否正确运行。如果 kube-proxy 一切正常,就应该查看宿主机的 iptables。\niptables 模式的话检查:\n\nKUBE-SERVICE 或者 KUBE-NODEPORTS 规则对应的入口链，这个规则应该和 VIP 和 Service 端口一一对应\nKUBE-SEP-(hash) 规则对应的 DNAT 链,和 Endpoint 一一对应\nKUBE-SVC-(hash) 规则对应负载均衡链，这些规则的数目应该和 Endpoint 数目对应\n如果是 NodePort 模式的话，还有 POSTROUTING 的 SNAT 链\n\nPod 没办法通过 Service 访问自己这往往就是因为 kubelet 的 hairpin-mode 没有正确被设置(Haripin Mode 发夹模式，在默认情况下，网桥设备是不允许一个数据包从一个端口进来，再从这个端口发出去，开启这个模式从而取消这个限制)\n调试技巧使用 nsenter 来进入 Pod 容器\nfunction e() &#123;    set -u    ns=$&#123;2-&quot;default&quot;&#125;    container_id=`kubectl -n $ns describe pod $1 | grep -Eo &#x27;docker://.*$&#x27; | head -n 1 | awk -F &#x27;//&#x27; &#x27;&#123;print $2&#125;&#x27;`    if [ &quot;$container_id&quot; != &quot;&quot; ]    then        echo &quot;container_id:$container_id&quot;        pid=`docker inspect -f &#123;&#123;.State.Pid&#125;&#125; $container_id`        echo &quot;pid:$pid&quot;        echo &quot;enter pod netns successfully for $ns/$1&quot;        nsenter -n --target $pid    fi&#125;\n\n在需要调试的 Pod 的宿主机上先执行这个命令，然后使用 e pod-name namespaces 来进入 Pod 进行调试\n真实案例复盘现象在集群内任意一个节点上访问 Service 的 cluster ip，如果这个 Service 的 Pod 运行在当前节点，则能访问成功，否则访问不成功\n分析在运行有 Pod 的宿主机上使用 tcpdump 抓 cni0 的包(cni0 和 eth0 做了桥接)，发现包正常请求，正常返回\n$ tcpdump -nn -i cni0 tcp and host 10.28.248.11\n\n分析：服务端是正常处理了请求的，应该是在返回的时候包被丢弃了(因为没有返回结果)。通过上面的 IPVS 模式的介绍，查看 iptables 的配置，发现了问题：\n# pod ClusterCIDR: 10.28.0.0/16# ServiceCIDR: 10.28.0.0/16\n\n而 iptables 处理的是非 10.28.0.0/16 的包才进行 SNAT，所以导致包回不去，被丢弃了。\n解决办法\n修改 ServiceCIDR 为另外一个网段(推荐)\n每个宿主机上手动添加一条规则: iptables -t nat -A POSTROUTING -s 10.28.0.0/16 -j SNAT --to-source 10.28.252.241(node ip)\n\n","categories":["CloudNative"],"tags":["Kubernetes","IPVS","iptables"]},{"title":"Linux 进程管理(一)：进程和线程","url":"/2022/07/30/os/linux_process_and_thread/","content":"Linux 进程和线程\n进程进程的定义如下:\n\n进程（Process）是指计算机已执行的程序，用户下达执行进程的命令后，就会产生进程。同一个程序可以产生多个进程（一对多），以允许同时有多个用户执行同一个程序，却不会相互冲突。进程需要一些资源才能完成工作，比如 CPU、内存、存储、文件以及 I&#x2F;O 设备等。\n\n这里说的程序，一般都是 ELF 格式的可执行文件。\nELFELF(Executable and Linking Format) 一般有这几种格式:\n\n可重定位文件\n可执行文件\n共享对象文件\n内核转储文件\n\n可重定位文件可重定位文件（Relocatable File）包含适用于与其他目标文件链接来创建可执行文件或者共享目标文件的代码和数据，一般都是 *.o 文件，这个编译好的二进制文件里面，应该是代码，还有一些全局变量、静态变量等等。\n\n为什么这里只有全局变量呢？是因为局部变量放到栈里面的，是程序运行过程中随时分配的空间，随时释放的，这里还是二进制文件，还没有启动，所以只有全局变量\n\n这个编译好的代码和变量，将来加载到内存里面的时候，都是要加载到一定位置的。比如调用一个函数，其实就是跳到这个函数所在的代码位置执行；如果修改全局变量，也要到变量的位置那里修改。但是在这个时候，还是 *.o 文件，不是一个可直接运行的程序。所以 *.o 里面的位置是不确定的，但是必须是可重新定位的，因为它将来是要做函数库的，加载到哪里就重新定位这些代码、变量的位置。\n可执行文件可执行文件（Executable File）包含适合执行的一个程序，此文件规定了 exec() 如何创建一个程序的进程映像，即 *.out 文件。格式基本上和 *.o 文件大致相似，只不过有多个 *.o 合并过的。这个文件是可以直接加载到内存里面执行的文件了，在 ELF 头里面，有一项 e_entry，也是个虚拟地址，是这个程序运行的入口。一般断点调试的时候可以第一个断点标记到这里\n$ docker run -it --rm --name centos centos:7.9.2009[root@fe87ccfdbd0a /]# readelf -h /usr/bin/lsELF Header:  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00  Class:                             ELF64  Data:                              2&#x27;s complement, little endian  Version:                           1 (current)  OS/ABI:                            UNIX - System V  ABI Version:                       0  Type:                              EXEC (Executable file)  Machine:                           AArch64  Version:                           0x1  Entry point address:               0x404aa8  Start of program headers:          64 (bytes into file)  Start of section headers:          136320 (bytes into file)  Flags:                             0x0  Size of this header:               64 (bytes)  Size of program headers:           56 (bytes)  Number of program headers:         9  Size of section headers:           64 (bytes)  Number of section headers:         30  Section header string table index: 29\n\n通过 readelf 可以查看到这个 Entry point address\n共享对象文件共享对象文件（Shared Object File）包含可在两种上下文中链接的代码和数据，首先链接编译器可以将它和其它重定位文件和动态链接库一起处理，生成另外一个目标文件。其次，动态链接器（Dynamic Linker）可能将它与某个可执行文件以及其它共享对象文件一起组合，创建进程映像。即 *.so 文件。\n内核转储内核转储（core dumps）存放当前进程的执行上下文，用于 dump 信号触发。\n知道了 ELF 这个格式，这个时候它还是一个程序，需要加载到内存里面。\n启动进程在内核中，有这样一个数据结构，用来定义加载二进制文件的方法:\ninclude/linux/binfmts.h/* * This structure defines the functions that are used to load the binary formats that * linux accepts. */struct linux_binfmt &#123;\tstruct list_head lh;\tstruct module *module;\tint (*load_binary)(struct linux_binprm *);\tint (*load_shlib)(struct file *);#ifdef CONFIG_COREDUMP\tint (*core_dump)(struct coredump_params *cprm);\tunsigned long min_coredump;\t/* minimal dump size */#endif&#125; __randomize_layout;\n\n对于 ELF 文件格式，有对应的实现\nfs/binfmt_elf.cstatic struct linux_binfmt elf_format = &#123;\t.module\t\t= THIS_MODULE,\t.load_binary\t= load_elf_binary,\t.load_shlib\t= load_elf_library,#ifdef CONFIG_COREDUMP\t.core_dump\t= elf_core_dump,\t.min_coredump\t= ELF_EXEC_PAGESIZE,#endif&#125;;\n\n可以看到最终是通过 load_elf_binary 加载的，这里就不继续深入了。\n创建进程使用代码来创建进程\nprocess.c#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/wait.h&gt;int main(int argc, char *argv[])&#123;\tprintf(&quot;hello world (pid:%d)\\n&quot;, (int)getpid());\tint rc = fork();\tif (rc &lt; 0) &#123;\t\tfprintf(stderr, &quot;fork failed\\n&quot;);\t\texit(1);\t&#125; else if (rc == 0)&#123;\t\tprintf(&quot;hello, I am child (pid:%d)\\n&quot;,(int)getpid());\t&#125; else &#123;\t\tint wc = wait(NULL);\t\tprintf(&quot;hello, I am parent of %d (wc:%d) (pid:%d)\\n&quot;, rc, wc, (int)getpid());\t&#125;\treturn 0;&#125;\n\n运行程序\n[root@ec2c598cce50 /]# gcc process.c -o process[root@ec2c598cce50 /]# ./processhello world (pid:115)hello, I am child (pid:116)hello, I am parent of 116 (wc:116) (pid:115)\n\n可以看到上面的父进程通过 fork 这个系统调用创建了子进程。如果在上面添加一个休眠，就可以通过 ps 查看到两个进程\nprocess.c#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;sys/wait.h&gt;int main(int argc, char *argv[])&#123;\tprintf(&quot;hello world (pid:%d)\\n&quot;, (int)getpid());\tint rc = fork();\tif (rc &lt; 0) &#123;\t\tfprintf(stderr, &quot;fork failed\\n&quot;);\t\texit(1);\t&#125; else if (rc == 0)&#123;\t\tprintf(&quot;hello, I am child (pid:%d)\\n&quot;,(int)getpid());+\t\tsleep(10);\t&#125; else &#123;\t\tint wc = wait(NULL);\t\tprintf(&quot;hello, I am parent of %d (wc:%d) (pid:%d)\\n&quot;, rc, wc, (int)getpid());\t&#125;\treturn 0;&#125;\n\n继续运行发现不会退出，因为 wait(NULL)，父进程需要等待子进程退出，所以通过 ps 查看进程\n[root@ec2c598cce50 /]# ps -efUID          PID    PPID  C STIME TTY          TIME CMDroot           1       0  0 5月28 ?        00:00:16 /usr/lib/systemd/systemd --switched-root --system --deserialize 21root           2       0  0 5月28 ?        00:00:00 [kthreadd]root           3       2  0 5月28 ?        00:00:00 [rcu_gp]root           4       2  0 5月28 ?        00:00:00 [rcu_par_gp]......root         148       1  0 09:10 pts/0    00:00:00 ./processroot         149     148  0 09:10 pts/0    00:00:00 ./processroot         150     117  0 09:10 pts/1    00:00:00 ps -ef\n\n可以看到 148、149 都是 process 这个进程，只不过，148 的 PPID 是 1，而 149 的 PPID 是 148（父进程）。还可以看到 1 号进程的父进程是 0 号进程，而 2 号进程的父进程也是 0 号进程。那么这里的 0,1,2 号进程是什么呢？在系统中有什么作用？\n特殊进程0, 1, 2 号进程是 Linux 系统初始化的时候创建的三个进程，初始化的函数在 start_kernel 这里\ninit/main.casmlinkage __visible void __init __no_sanitize_address start_kernel(void)&#123;\tchar *command_line;\tchar *after_dashes;\tset_task_stack_end_magic(&amp;init_task);\tsmp_setup_processor_id();\tdebug_objects_early_init();\tinit_vmlinux_build_id();\tcgroup_init_early();// ...... 省略其它逻辑\t/* Do the rest non-__init&#x27;ed, we&#x27;re now alive */\tarch_call_rest_init();\tprevent_tail_call_optimization();&#125;\n\n0 号进程通过 init/main.c 的 start_kernel 代码，可以看到有一行 set_task_stack_end_magic(&amp;init_task)，其中 init_task 定义如下：\ninit/init_task.c/* * Set up the first task table, touch at your own risk!. Base=0, * limit=0x1fffff (=2MB) */struct task_struct init_task\n\n它是系统创建的第一个进程，称为 0 号进程，这是唯一一个没有通过 fork 或者 kernel_thread 产生的进程，是进程列表的第一个。\n1 号进程通过 init/main.c 的 start_kernel 代码，可以看到有一行 arch_call_rest_init()\ninit/main.cvoid __init __weak arch_call_rest_init(void)&#123;\trest_init();&#125;noinline void __ref rest_init(void)&#123;\tstruct task_struct *tsk;\tint pid;\trcu_scheduler_starting();\t/*\t * We need to spawn init first so that it obtains pid 1, however\t * the init task will end up wanting to create kthreads, which, if\t * we schedule it before we create kthreadd, will OOPS.\t */\tpid = user_mode_thread(kernel_init, NULL, CLONE_FS);\t/*\t * Pin init on the boot CPU. Task migration is not properly working\t * until sched_init_smp() has been run. It will set the allowed\t * CPUs for init to the non isolated CPUs.\t */ \trcu_read_lock();\ttsk = find_task_by_pid_ns(pid, &amp;init_pid_ns);\ttsk-&gt;flags |= PF_NO_SETAFFINITY;\tset_cpus_allowed_ptr(tsk, cpumask_of(smp_processor_id()));\trcu_read_unlock();\tnuma_default_policy();\tpid = kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES);\t// ......&#125;\n\n可以看到 arch_call_rest_init 最终调用了 rest_init，rest_init 第一大工作就是使用 user_mode_thread(kernel_init, NULL, CLONE_FS) 创建第二个进程，这个就是 1 号进程。对于 Linux 操作系统来说，有“划时代”的意义。因为它将运行一个用户进程，这意味着后续的用户进程可以通过这个进程创建，从而形成一颗进程树。可以看到传入的第一个参数 kernel_init 是一个函数\ninit/main.cstatic char *ramdisk_execute_command = &quot;/init&quot;;static int __ref kernel_init(void *unused)&#123;\t// ......\tif (ramdisk_execute_command) &#123;\t\tret = run_init_process(ramdisk_execute_command);\t\tif (!ret)\t\t\treturn 0;\t\tpr_err(&quot;Failed to execute %s (error %d)\\n&quot;,\t\t       ramdisk_execute_command, ret);\t&#125;\t/*\t * We try each of these until one succeeds.\t *\t * The Bourne shell can be used instead of init if we are\t * trying to recover a really broken machine.\t */\tif (execute_command) &#123;\t\tret = run_init_process(execute_command);\t\tif (!ret)\t\t\treturn 0;\t\tpanic(&quot;Requested init %s failed (error %d).&quot;,\t\t      execute_command, ret);\t&#125;\tif (CONFIG_DEFAULT_INIT[0] != &#x27;\\0&#x27;) &#123;\t\tret = run_init_process(CONFIG_DEFAULT_INIT);\t\tif (ret)\t\t\tpr_err(&quot;Default init %s failed (error %d)\\n&quot;,\t\t\t       CONFIG_DEFAULT_INIT, ret);\t\telse\t\t\treturn 0;\t&#125;\tif (!try_to_run_init_process(&quot;/sbin/init&quot;) ||\t    !try_to_run_init_process(&quot;/etc/init&quot;) ||\t    !try_to_run_init_process(&quot;/bin/init&quot;) ||\t    !try_to_run_init_process(&quot;/bin/sh&quot;))\t\treturn 0;\tpanic(&quot;No working init found.  Try passing init= option to kernel. &quot;\t      &quot;See Linux Documentation/admin-guide/init.rst for guidance.&quot;);&#125;static int run_init_process(const char *init_filename)&#123;\tconst char *const *p;\targv_init[0] = init_filename;\tpr_info(&quot;Run %s as init process\\n&quot;, init_filename);\tpr_debug(&quot;  with arguments:\\n&quot;);\tfor (p = argv_init; *p; p++)\t\tpr_debug(&quot;    %s\\n&quot;, *p);\tpr_debug(&quot;  with environment:\\n&quot;);\tfor (p = envp_init; *p; p++)\t\tpr_debug(&quot;    %s\\n&quot;, *p);\treturn kernel_execve(init_filename, argv_init, envp_init);&#125;static int try_to_run_init_process(const char *init_filename)&#123;\tint ret;\tret = run_init_process(init_filename);\tif (ret &amp;&amp; ret != -ENOENT) &#123;\t\tpr_err(&quot;Starting init: %s exists but couldn&#x27;t execute it (error %d)\\n&quot;,\t\t       init_filename, ret);\t&#125;\treturn ret;&#125;\n\n可以看到调用过程，先尝试运行 ramdisk 的 /init，或者普通文件系统上的 /sbin/init, /etc/init, /bin/init, /bin/sh（不同版本的 Linux 会选择不同的文件启动），只要有一个启动起来就行了。\n最终调用的流程:\nflowchart LR\n\nkernel_execve --&gt; bprm_execve\nbprm_execve --&gt; exec_binprm\nexec_binprm --&gt; search_binary_handler\nsearch_binary_handler --&gt; load_binary\n\n可以看到 load_binary 的 ELF 实现就是 load_elf_library，最终调用 start_thread\narch/x86/kernel/process_32.cvoidstart_thread(struct pt_regs *regs, unsigned long new_ip, unsigned long new_sp)&#123;\tloadsegment(gs, 0);\tregs-&gt;fs\t\t= 0;\tregs-&gt;ds\t\t= __USER_DS;\tregs-&gt;es\t\t= __USER_DS;\tregs-&gt;ss\t\t= __USER_DS;\tregs-&gt;cs\t\t= __USER_CS;\tregs-&gt;ip\t\t= new_ip;\tregs-&gt;sp\t\t= new_sp;\tregs-&gt;flags\t\t= X86_EFLAGS_IF;&#125;EXPORT_SYMBOL_GPL(start_thread);\n\n这里的 pt_regs 就是寄存器，这个结构就是在系统调用的时候，内核中保存用户态上下文的，里面将用户态的代码段 CS 设置为 __USER_CS，将用户态的数据段 DS 设置为 __USER_DS，以及指令指针寄存器 IP、栈指针寄存器 SP，这里相当于补上了系统调用里，保存寄存器的步骤。所以这里执行完成之后，就回到用户态了。所以是相当于从内核态执行了 exec 生成了 1 号进程，执行完成后，1 号进程从内核态返回用户态并最终运行在用户态，成为了用户态进程的祖先。\n2 号进程通过 init/main.c 的 rest_init 第二个工作就是通过 kernel_thread(kthreadd, NULL, CLONE_FS | CLONE_FILES) 创建 2 号进程，这里的 thread 可以翻译成线程，这也是 Linux 操作系统很重要的一个概念。2 号进程管理所有的内核态的进程，kthreadd 负责所有内核态的线程的调度和管理，是内核态所有线程运行的祖先。\n线程从用户态来看，进程就是把一个可执行程序启动起来，这个进程包含很多资源，一般一个进程需要有多个模块执行不同或相同的任务，这就叫多线程（Multithreading）。如果只有一个线程，那它就是进程的主线程（一般项目的 main 函数所在的线程）。但是从内核态来看，无论是进程还是线程，都可以统称为任务（Task），都使用相同的数据结构，放在同一个链表中。\n为什么要有线程其实对于人一个进程来讲，即便没有主动创建线程，进程也是默认有一个主线程的。线程是负责执行二进制指令的，进程要比线程管的多，除了要执行指令外，内存、文件系统等都需要进程来管理。进程相当于一个项目，而线程就是为了完成项目需求，而建立的一个个开发任务\n创建线程thread.c#include &lt;stdio.h&gt;#include &lt;assert.h&gt;#include &lt;pthread.h&gt;static volatile int counter = 0;static pthread_mutex_t lock;void *mythread(void *arg) &#123;\tprintf(&quot;%s: begin\\n&quot;, (char *)arg);\tint i;\tpthread_mutex_lock(&amp;lock);\tfor (i = 0; i &lt; 1e7; i++) &#123;\t\tcounter++;\t&#125;\tpthread_mutex_unlock(&amp;lock);\tprintf(&quot;%s: done\\n&quot;, (char *)arg);\tpthread_exit((void *)mythread);&#125;int main(int argc, char *argv[]) &#123;\tpthread_t p1, p2;\tint rc;\trc = pthread_mutex_init(&amp;lock, NULL); assert(rc == 0);\tpthread_attr_t thread_attr;                                        \tpthread_attr_init(&amp;thread_attr);                                   \tpthread_attr_setdetachstate(&amp;thread_attr, PTHREAD_CREATE_JOINABLE);\tprintf(&quot;main: begin (counter = %d)\\n&quot;, counter);\trc = pthread_create(&amp;p1, &amp;thread_attr, mythread, &quot;A&quot;); assert(rc == 0);\trc = pthread_create(&amp;p2, &amp;thread_attr, mythread, &quot;B&quot;); assert(rc == 0);\tpthread_attr_destroy(&amp;thread_attr);\t// join waits for the threads to finish\trc = pthread_join(p1, NULL); assert(rc == 0);\trc = pthread_join(p2, NULL); assert(rc == 0);\tprintf(&quot;main: done with both (counter = %d)\\n&quot;, counter);\tpthread_mutex_destroy(&amp;lock);\tpthread_exit(NULL);&#125;\n\n由于 pthread 库不是标准 Linux 库，所以需要使用 gcc thread.c -l pthread 来编译\n[root@ec2c598cce50 tmp]# gcc thread.c -l pthread[root@ec2c598cce50 tmp]# ./a.outmain: begin (counter = 0)B: beginA: beginB: doneA: donemain: done with both (counter = 20000000)\n\n一个普通线程的创建和运行过程:\n\n声明线程函数\nvoid *mythread(void *arg) {……pthread_exit((void *)output);}\n\n\n声明线程对象\npthread_t p1, p2;\n\n\n设置线程属性\npthread_attr_t thread_attr;pthread_attr_init(&amp;thread_attr);pthread_attr_setdetachstate(&amp;thread_attr, PTHREAD_CREATE_JOINABLE);\n\n\n创建线程\nrc &#x3D; pthread_create(&amp;p1, &amp;thread_attr, mythread, “A”); assert(rc &#x3D;&#x3D; 0);\n\n\n子线程开始\nvoid *mythread(void *arg)\n\n\n子线程结束\npthread_exit\n\n\n\n\n销毁线程属性\npthread_attr_destroy(&amp;thread_attr);\n\n\n等待线程结束\nrc &#x3D; pthread_join(p1, NULL); assert(rc &#x3D;&#x3D; 0);\n\n\n主线程结束\npthread_exit(NULL);\n\n\n\n数据线程可以将进程并行起来，加快进度，但是也会带来负面影响，过程并行了，数据应该如何“并行”？\n本地数据线程上的本地数据，比如函数执行过程中的局部变量。每个线程都有自己的栈空间，栈的大小可以通过命令 ulimit -a 查看\n[root@ec2c598cce50 tmp]# ulimit -acore file size          (blocks, -c) 0data seg size           (kbytes, -d) unlimitedscheduling priority             (-e) 0file size               (blocks, -f) unlimitedpending signals                 (-i) 31345max locked memory       (kbytes, -l) 64max memory size         (kbytes, -m) unlimitedopen files                      (-n) 1048576pipe size            (512 bytes, -p) 8POSIX message queues     (bytes, -q) 819200real-time priority              (-r) 0stack size              (kbytes, -s) 8192cpu time               (seconds, -t) unlimitedmax user processes              (-u) unlimitedvirtual memory          (kbytes, -v) unlimitedfile locks                      (-x) unlimited\n\n默认 stack size 大小为 8192（8MB），可以通过 ulimit -s 修改。对于线程栈，可以通过 pthread_attr_t 修改线程栈的大小\nint pthread_attr_setstacksize(pthread_attr_t *attr, size_t stacksize);\n\n主线程在内存中有一个栈空间，其它线程也拥有独立的栈空间。为了避免线程之间的栈空间踩踏，线程栈之间还会有一小块区域，用来隔离保护各自的栈空间。一旦另一个线程踏入到这个隔离区，就会引发段错误(SegmentFault)。\n线程函数调用的压栈过程：\n\n全局数据在整个进程里共享的全局数据，例如全局变量，虽然在不同进程中是隔离的，但是在一个进程中是共享的。如果同一个全局变量，两个线程一起修改，那肯定会有问题。这就需要有一种机制来保护顺序，让运行结果可预期。可以通过加锁或者信号量等一些办法来控制。\n私有数据可以通过 pthread_key_create 创建\nint pthread_key_create(pthread_key_t *key, void (*destructor)(void*))\n\n创建一个 key，的同时，会传入一个析构函数（释放资源的时候执行的函数）。key 一旦被创建，所欲呕线程都可以访问它，但各个线程可以根据自己的需要往 key 中填入不同的值，这就相当于提供了一个同名而不同值的全局变量。\n// 设置 key 对应的 valueint pthread_setspecific(pthread_key_t key, const void *value)// 获取 key 对应的 valuevoid *pthread_getspecific(pthread_key_t key)\n\n\n在 Linux 里面，无论是进程还是线程，到了内核里面，统一都叫任务（Task），由一个统一的结构体 task_struct 进行管理。接下来将继续学习任务的调度。\n\n\n参考链接:\n\nhttps://mp.weixin.qq.com/s/HFQL5d-C24DKjQbYAQfl8g\nhttps://mp.weixin.qq.com/s?__biz&#x3D;Mzk0MjE3NDE0Ng&#x3D;&#x3D;&amp;mid&#x3D;2247501576&amp;idx&#x3D;1&amp;sn&#x3D;657e1c89d1007a935820d394d64f4035&amp;scene&#x3D;21#wechat_redirect\nhttps://zhuanlan.zhihu.com/p/79772089\nhttps://www.jianshu.com/p/0d90b92000c0\nhttps://segmentfault.com/a/1190000039367851?utm_source&#x3D;sf-similar-article\nhttps://www.cnblogs.com/chaojiyingxiong/p/15799617.html\nhttp://www.wowotech.net/process_management&#x2F;scheduler-history.html\nhttps://www.ibm.com/developerworks/cn/linux/l-cn-scheduler/index.html\nhttps://www.jianshu.com/p/673c9e4817a8\nhttps://blog.eastonman.com/blog/2021/02/cfs/\nhttps://www.kernel.org/doc/html/latest/scheduler/sched-design-CFS.html\nhttps://time.geekbang.org/column/intro/100024701?tab=catalog\n\n","categories":["OS"],"tags":["Linux"]},{"title":"Linux 进程管理(二)：任务调度","url":"/2022/07/31/os/linux_task_scheduling/","content":"Linux 任务调度\ntask_struct在 Linux 里面，无论是进程还是线程，到了内核里面，统一都叫任务（Task），由一个统一的结构体 task_struct 进行管理\n\n这个结构有点复杂，具体可以看相关 代码。\n任务 ID每一个任务都应该有一个任务 ID 作为唯一标识\ncode\ninclude/linux/sched.hpid_t\t\t\t\tpid;pid_t\t\t\t\ttgid;// ....../* Real parent process: */struct task_struct __rcu\t*real_parent;/* Recipient of SIGCHLD, wait4() reports: */struct task_struct __rcu\t*parent;/*  * Children/sibling form the list of natural children:  */struct list_head\t\tchildren;struct list_head\t\tsibling;struct task_struct\t\t*group_leader;\n\n\npid（process ID）: 线程 ID\ntgid（thread group ID）：主线程 ID，当前线程就是主线程，那么 tgid 就是 pid\nparent: 指向其父进程。当它终止时，必须向它的父进程发送信号\nchildren: 表示链表的头部。链表中的所有元素都是它的子进程\nsibling: 用于把当前进程插入到兄弟链表中\ngroup_leader：指向进程的主线程\n\n通过对比 tgid 和 pid 就可以知道当前 task_struct 是进程还是线程。如果只是想通过 ps 查看有多少进程，可以只展示 pid == tgid 的 task_struct，就不用把所有的都展示出来；如果想要结束一个进程，可以使用 kill 命令结束整个进程而不是线程。\n\n当然也有发送信号给单个线程的情况：可以看 Go Runtime 的抢占式调度的实现\n\n状态在 task_struct 的状态 code\ninclude/linux/sched.h/* Used in tsk-&gt;state: */#define TASK_RUNNING\t\t\t0x0000#define TASK_INTERRUPTIBLE\t\t0x0001#define TASK_UNINTERRUPTIBLE\t\t0x0002#define __TASK_STOPPED\t\t\t0x0004#define __TASK_TRACED\t\t\t0x0008/* Used in tsk-&gt;exit_state: */#define EXIT_DEAD\t\t\t0x0010#define EXIT_ZOMBIE\t\t\t0x0020#define EXIT_TRACE\t\t\t(EXIT_ZOMBIE | EXIT_DEAD)/* Used in tsk-&gt;state again: */#define TASK_PARKED\t\t\t0x0040#define TASK_DEAD\t\t\t0x0080#define TASK_WAKEKILL\t\t\t0x0100#define TASK_WAKING\t\t\t0x0200#define TASK_NOLOAD\t\t\t0x0400#define TASK_NEW\t\t\t0x0800/* RT specific auxilliary flag to mark RT lock waiters */#define TASK_RTLOCK_WAIT\t\t0x1000#define TASK_STATE_MAX\t\t\t0x2000/* Convenience macros for the sake of set_current_state: */#define TASK_KILLABLE\t\t\t(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)#define TASK_STOPPED\t\t\t(TASK_WAKEKILL | __TASK_STOPPED)#define TASK_TRACED\t\t\t__TASK_TRACED#define TASK_IDLE\t\t\t(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)/* Convenience macros for the sake of wake_up(): */#define TASK_NORMAL\t\t\t(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)/* get_task_state(): */#define TASK_REPORT\t\t\t(TASK_RUNNING | TASK_INTERRUPTIBLE | \\\t\t\t\t\t TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \\\t\t\t\t\t __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \\\t\t\t\t\t TASK_PARKED)\n\nTASK_RUNNING 并不是说进程正在运行，而是表示进程在时刻准备运行的状态。当处于这个状态的进程获得时间片的时候，就是在运行中；如果没有获得时间片，就说明它被其他进程抢占了，等待再次分配时间片。运行中的进程，一旦要进行一些 I&#x2F;O 操作，需要等待 I&#x2F;O 操作完毕，这个时候会主动释放 CPU，进入睡眠状态，在 Linux 中，有两种睡眠状态\n\nTASK_INTERRUPTIBLE：可中断的睡眠状态。虽然在睡眠，等待 I&#x2F;O 完成，但是这个时候一个信号来的时候，进程还是要被唤醒，只是唤醒之后不是继续刚才的操作，而是进行信号处理；\nTASK_UNINTERRUPTIBLE：不可中断的睡眠状态。只能等待 I&#x2F;O 完成，连 kill 信号也会被忽略。除非重启电脑，没有其他办法；\nTASK_KILLABLE：可终止的睡眠状态。进程处于这种状态中，原理和 TASK_UNINTERRUPTIBLE 类似，只不过是可以响应 kill 信号；\nTASK_STOPPED：在进程收到 SIGSTOP、SIGTTIN、SIGTSTP 或者 SIGTTOU 信号之后进入该状态；\nTASK_TRACED：表示进程被 debugger 等进程监控，进程执行被调试程序所停止。当一个进程被另外的进程所监视，每一个信号都会让该进程进入这个状态；\nEXIT_ZOMBIE：一旦一个进程要结束，先进入这个状态，但是这个时候它的父进程还没有使用 wait() 等系统调用来获取它的终止信息，此时进程就成了僵尸进程；\nEXIT_DEAD：是进程的最终状态\n\n状态转换如下所示：\nstateDiagram\n\nstate &quot;p-&gt;state = TASK_RUNNING&quot; as pstate\nstate &quot;运行&quot; as run\n\n[*] --&gt;  sys_fork\nsys_fork --&gt; _do_fork\n_do_fork --&gt; wake_up_new_task\nwake_up_new_task --&gt; pstate\npstate --&gt; TASK_RUNNING\nTASK_RUNNING --&gt; run: 分配 CPU\nrun --&gt; TASK_RUNNING: 时间片使用完毕或被抢占\nrun --&gt; TASK_INTERRUPTIBLE\nrun --&gt; TASK_UNINTERRUPTIBLE\nrun --&gt; TASK_KILLABLE\nTASK_INTERRUPTIBLE --&gt; TASK_RUNNING\nTASK_UNINTERRUPTIBLE --&gt; TASK_RUNNING\nTASK_KILLABLE --&gt; TASK_RUNNING\nrun --&gt; TASK_STOPPED\nrun --&gt; EXIT_ZOMBIE: exit\nEXIT_ZOMBIE --&gt; EXIT_DEAD\nEXIT_DEAD --&gt; [*]\n\n运行统计信息主要包含在下面这些字段中 code\ninclude/linux/sched.h\tu64\t\t\t\tutime;  // 用户态消耗的 CPU 时间\tu64\t\t\t\tstime;  // 内核态消耗的 CPU 时间#ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME\tu64\t\t\t\tutimescaled;\tu64\t\t\t\tstimescaled;#endif\tu64\t\t\t\tgtime;\tstruct prev_cputime\t\tprev_cputime;#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN\tstruct vtime\t\t\tvtime;#endif#ifdef CONFIG_NO_HZ_FULL\tatomic_t\t\t\ttick_dep_mask;#endif\t/* Context switch counts: */\tunsigned long\t\t\tnvcsw;  // 自愿(voluntary)上下文切换计数\tunsigned long\t\t\tnivcsw; // 非自愿(involuntary)上下文切换计数\t/* Monotonic time in nsecs: */\tu64\t\t\t\tstart_time;     // 进程启动时间，不包含睡眠时间\t/* Boot based time in nsecs: */\tu64\t\t\t\tstart_boottime; // 进程启动时间，包含睡眠时间\n\n调度原理在 Linux 里面，进程可以分为两种：实时进程和普通进程。\n\n实时进程：需要尽快执行返回结果的那种\n普通进程：大部分进程其实都是这种\n\n调度策略在 task_struct 中 的 policy 用来表示调度策略，有以下一些策略 code\ninclude/uapi/linux/sched.h/* * Scheduling policies */#define SCHED_NORMAL\t\t0  // SCHED_OTHER#define SCHED_FIFO\t\t1#define SCHED_RR\t\t2#define SCHED_BATCH\t\t3/* SCHED_ISO: reserved but not implemented yet */#define SCHED_IDLE\t\t5#define SCHED_DEADLINE\t\t6\n\n配合调度策略的，还有优先级\ninclude/linux/sched.hint\t\t\t\tprio;int\t\t\t\tstatic_prio;int\t\t\t\tnormal_prio;unsigned int\t\t\trt_priority;\n\n优先级其实就是一个数值，对于实时进程，优先级范围是 0～99；对于普通进程，优先级范围是 100~139。数值越小，优先级越高。\n实时调度策略SCHED_FIFO、SCHED_RR、SCHED_DEADLINE 是实时进程的调度策略\n\nSCHED_FIFO：先来先执行\nSCHED_RR：轮流调度，采用时间片，相同优先级的任务当用完时间片会被放到队列尾部，以保证公平性，而更高优先级的任务也是可以抢占低优先级的任务\nSCHED_DEADLINE：按照任务的 deadline 进行调度的。DL 调度器总是选择其 deadline 距离当前时间点最近的那个任务，并调度它执行\n\n普通调度策略SCHED_NORMAL、SCHED_BATCH、SCHED_IDLE 是普通进程的调度策略\n\nSCHED_NORMAL: 普通进程一般都是这个调度策略\nSCHED_BATCH: 后台进程，几乎不需要和前段进行交互\nSCHED_IDLE: 特别空闲的时候才跑的进程\n\n修改&#x2F;查看调度策略!690 # chrt -p 3986612pid 3986612&#x27;s current scheduling policy: SCHED_OTHERpid 3986612&#x27;s current scheduling priority: 0!698 # chrt  -f -p 10 3986612pid 3986612&#x27;s current scheduling policy: SCHED_FIFOpid 3986612&#x27;s current scheduling priority: 10!692 # chrtShow or change the real-time scheduling attributes of a process.Set policy: chrt [options] &lt;priority&gt; &lt;command&gt; [&lt;arg&gt;...] chrt [options] --pid &lt;priority&gt; &lt;pid&gt;Get policy: chrt [options] -p &lt;pid&gt;Policy options: -b, --batch          set policy to SCHED_BATCH -d, --deadline       set policy to SCHED_DEADLINE -f, --fifo           set policy to SCHED_FIFO -i, --idle           set policy to SCHED_IDLE -o, --other          set policy to SCHED_OTHER -r, --rr             set policy to SCHED_RR (default)Scheduling options: -R, --reset-on-fork       set SCHED_RESET_ON_FORK for FIFO or RR -T, --sched-runtime &lt;ns&gt;  runtime parameter for DEADLINE -P, --sched-period &lt;ns&gt;   period parameter for DEADLINE -D, --sched-deadline &lt;ns&gt; deadline parameter for DEADLINEOther options: -a, --all-tasks      operate on all the tasks (threads) for a given pid -m, --max            show min and max valid priorities -p, --pid            operate on existing given pid -v, --verbose        display status information -h, --help     display this help and exit -V, --version  output version information and exitFor more details see chrt(1).\n\n不管是 policy 还是 priority，都是一个变量，真正做调度的是 sched_class\n调度策略实现sched_class 有这些实现 code\nkernel/sched/sched.hextern const struct sched_class stop_sched_class;extern const struct sched_class dl_sched_class;extern const struct sched_class rt_sched_class;extern const struct sched_class fair_sched_class;extern const struct sched_class idle_sched_class;\n\n\nstop_sched_class: 优先级最高的任务会使用这种策略，会中断所有其他线程，且不会被其他任务打断\ndl_sched_class: 对应 deadline 调度策略\nrt_sched_class: 对应 RR 算法或者 FIFO 算法的调度策略，具体调度策略由 task_struct-&gt;policy 指定\nfair_sched_class: 普通进程的调度策略\nidle_sched_class: 空闲进程的调度策略\n\n这里实时进程的调度策略 RR 和 FIFO 相对简单一些，平时遇到的基本上都是普通进程，这里重点分析普通进程的调度，也就是 fair_sched_class。\nCFS在 Linux 里面，实现了一个基于 CFS(Completely Fair Scheduling) 的调度算法。CPU 会提供一个时钟，过一段时间就触发一个时钟中断。CFS 为每个进程设置一个虚拟运行时间 vruntime。如果一个进程在运行，随着时间的增加，也就是一个个 tick 的到来，进程的 vruntime 将不断增大。没有得到运行的进程 vruntime 不变。vruntime 小的进程，会优先运行进程。code\nkernel/sched/fair.c/* * Update the current task&#x27;s runtime statistics. */static void update_curr(struct cfs_rq *cfs_rq)&#123;\tstruct sched_entity *curr = cfs_rq-&gt;curr;\tu64 now = rq_clock_task(rq_of(cfs_rq));\tu64 delta_exec;\tif (unlikely(!curr))\t\treturn;\tdelta_exec = now - curr-&gt;exec_start;\tif (unlikely((s64)delta_exec &lt;= 0))\t\treturn;\tcurr-&gt;exec_start = now;\t// ......\tcurr-&gt;vruntime += calc_delta_fair(delta_exec, curr);\tupdate_min_vruntime(cfs_rq);\t// ......&#125;/* * delta /= w */static inline u64 calc_delta_fair(u64 delta, struct sched_entity *se)&#123;\tif (unlikely(se-&gt;load.weight != NICE_0_LOAD))\t\tdelta = __calc_delta(delta, NICE_0_LOAD, &amp;se-&gt;load);\treturn delta;&#125;// include/linux/sched.hstruct sched_entity &#123;\t/* For load-balancing: */\tstruct load_weight\t\tload;\tstruct rb_node\t\t\trun_node;\tstruct list_head\t\tgroup_node;\tunsigned int\t\t\ton_rq;\tu64\t\t\t\texec_start;\tu64\t\t\t\tsum_exec_runtime;\tu64\t\t\t\tvruntime;\tu64\t\t\t\tprev_sum_exec_runtime;\t// ......&#125;;\n\n开始运行的时间减去当前时间，就是这次运行的时间 delta_exec，然后通过 calc_delta_fair 计算得到最终的 vruntime\n\nvruntime +&#x3D; 真实运行的时间 * NICE_0_LOAD &#x2F; 进程权重\n\n进程每次运行完毕后就会更新 vruntime，然后选择 vruntime 最小的进程继续运行，这将由红黑树完成。\n调度队列与调度实体上面说到的红黑树的节点，应该是包括 vruntime 的，称为调度实体，在 task_struct 中，有这些变量\ninclude/linux/sched.h// task_structstruct sched_entity\t\tse;\t\t// CFS 公平算法调度实体struct sched_rt_entity\t\trt; // 实时调度实体struct sched_dl_entity\t\tdl;\t// Deadline 调度实体struct sched_entity &#123;\t/* For load-balancing: */\tstruct load_weight\t\tload;\tstruct rb_node\t\t\trun_node;\tstruct list_head\t\tgroup_node;\tunsigned int\t\t\ton_rq;\tu64\t\t\t\texec_start;\tu64\t\t\t\tsum_exec_runtime;\tu64\t\t\t\tvruntime;\tu64\t\t\t\tprev_sum_exec_runtime;\tu64\t\t\t\tnr_migrations;\t// ......&#125;;\n\n进程根据自己是实时的，还是普通的，通过变量将自己挂到某一个调度队列里面，和其它进程排序，等待被调度。如果是普通进程，就将 sched_entity 挂到红黑树上。而真正维护这些队列的就是 CPU，所以每个 CPU 都有自己的 struct rq 结构\nkernel/sched/sched.hstruct rq &#123;\t/* runqueue lock: */\traw_spinlock_t\t\t__lock;\t/*\t * nr_running and cpu_load should be in the same cacheline because\t * remote CPUs use both these fields when doing load calculation.\t */\tunsigned int\t\tnr_running;\t// ......\tstruct cfs_rq\t\tcfs;\t// CFS 运行队列\tstruct rt_rq\t\trt;\t\t// 实时进程运行队列\tstruct dl_rq\t\tdl;\t\t// Deadline 运行队列\t// ......\t/*\t * This is part of a global counter where only the total sum\t * over all CPUs matters. A task can increase this counter on\t * one CPU and if it got migrated afterwards it may decrease\t * it on another CPU. Always updated under the runqueue lock:\t */\tunsigned int\t\tnr_uninterruptible;\tstruct task_struct __rcu\t*curr;\tstruct task_struct\t*idle;\tstruct task_struct\t*stop;\t// ......&#125;;\n\n对于普通进程公平队列 cfs_rq 中的 tasks_timeline 就是指向红黑树的根节点。调度的时候会先判断是否有实时进程需要运行，如果没有，才会去 CFS 运行队列面找 vruntime 最小的进程运行\nkernel/sched/core.c/* * Pick up the highest-prio task: */static inline struct task_struct *__pick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)&#123;\tconst struct sched_class *class;\tstruct task_struct *p;\t/*\t * Optimization: we know that if all tasks are in the fair class we can\t * call that function directly, but only if the @prev task wasn&#x27;t of a\t * higher scheduling class, because otherwise those lose the\t * opportunity to pull in more work from other CPUs.\t */\tif (likely(!sched_class_above(prev-&gt;sched_class, &amp;fair_sched_class) &amp;&amp;\t\t   rq-&gt;nr_running == rq-&gt;cfs.h_nr_running)) &#123;\t\tp = pick_next_task_fair(rq, prev, rf);\t\tif (unlikely(p == RETRY_TASK))\t\t\tgoto restart;\t\t/* Assume the next prioritized class is idle_sched_class */\t\tif (!p) &#123;\t\t\tput_prev_task(rq, prev);\t\t\tp = pick_next_task_idle(rq);\t\t&#125;\t\treturn p;\t&#125;restart:\tput_prev_task_balance(rq, prev, rf);\tfor_each_class(class) &#123;\t\tp = class-&gt;pick_next_task(rq);\t\tif (p)\t\t\treturn p;\t&#125;\tBUG(); /* The idle class should always have a runnable task. */&#125;\n\n这里的 for_each_class 就是 依次 循环，调用每个调度类的方法。这样整个运行的场景就可以串起来了，在每个 CPU 上都有一个队列 rq，这个队列包含了不同优先级的队列：rt_rq、cfs_rq 等，不同队列实现方式不一样，当某个 CPU 需要找下一个任务执行的时候，会按照优先级依次调用调度类，第一个没有找到才能找下一个调度类的任务，最终找到下个任务继续运行，这样就能保证实时任务的优先级永远大于普通任务。\n调度计算机主要处理计算、网络、存储三个方面。计算主要是 CPU 和内存的合作；网络和存储则多是和外部设备合作；在操作外部设备的时候，往往需要让出 CPU，调用 schedule() 函数\nkernel/sched/core.casmlinkage __visible void __sched schedule(void)&#123;\tstruct task_struct *tsk = current;\tsched_submit_work(tsk);\tdo &#123;\t\tpreempt_disable();\t\t__schedule(SM_NONE);\t\tsched_preempt_enable_no_resched();\t&#125; while (need_resched());\tsched_update_worker(tsk);&#125;EXPORT_SYMBOL(schedule);\n\nschedule 的主要逻辑都是在 __schedule 里面实现的\n__scheduleflowchart\n\nschedule --&gt; __schedule\n__schedule -- 1 --&gt; pick_next_task\npick_next_task --&gt; __pick_next_task\n__pick_next_task --&gt; pick_next_task_fair\n__pick_next_task --&gt; other[class-&gt;pick_next_task]\nother --&gt; other-logic\npick_next_task_fair --&gt; 1.update_curr\n1.update_curr --&gt; update-vruntime\npick_next_task_fair --&gt; 2.pick_next_entity\n2.pick_next_entity --&gt; __pick_first_entity\n__pick_first_entity -- 2 return next --&gt; __schedule\n__schedule -- 3 --&gt; context_switch\n\n\n取出当前 CPU 上的任务队列 rq，以及正在运行的进程 curr\n获取下一个要执行的任务\n如果是 CFS，就要更新 vruntime，然后取红黑树最左边的节点(vruntime 最小的任务)\n判断当前进程和下一个要执行的任务是否是同一个任务，如果不是，调用 context_switch\n\ncontext_switch 主要的逻辑就是上下文切换\n\n切换进程空间，即虚拟内存\n切换寄存器和 CPU 上下文\n\ntask_struct 里面有一个 thread 的变量，保留了要切换进程的时候需要修改的寄存器。\n进程调度都会走到 __schedule 这里，如果是主动调用 schedule 就是主动让出，那如果不主动让出呢？\n抢占式调度抢占过程最常见的现象就是一个进程执行时间太长，不主动让出 CPU，这个时候不能一直让这个进程继续运行，需要切换到另外一个进程。衡量这个时间点，就是计算机里面的时钟，每过一段时间（调度周期）就会触发一次时钟中断，这个时候 CPU 会切换任务去响应这个时钟中断，这个时候就可以查看是否需要抢占。时钟中断处理函数主要是 scheduler_tick\nflowchart LR\n\nscheduler_tick --&gt; task_tick\ntask_tick -- 根据 sched_class 判断 --&gt; task_tick_fair\ntask_tick_fair --&gt; entity_tick\nentity_tick --&gt; 1.update_curr\nentity_tick --&gt; 2.check_preempt_tick\n\n主要的就是 check_preempt_tick，检查是否是应该抢占当前进程\nkernel/sched/fair.c/* * Preempt the current task with a newly woken task if needed: */static voidcheck_preempt_tick(struct cfs_rq *cfs_rq, struct sched_entity *curr)&#123;\tunsigned long ideal_runtime, delta_exec;\tstruct sched_entity *se;\ts64 delta;  // ideal_runtime 一个调度周期中，运行的“实际”(理论)时间\tideal_runtime = sched_slice(cfs_rq, curr);  // delta_exec 这次调度运行的时间\tdelta_exec = curr-&gt;sum_exec_runtime - curr-&gt;prev_sum_exec_runtime;\tif (delta_exec &gt; ideal_runtime) &#123;\t\tresched_curr(rq_of(cfs_rq));\t\t/*\t\t * The current task ran long enough, ensure it doesn&#x27;t get\t\t * re-elected due to buddy favours.\t\t */\t\tclear_buddies(cfs_rq, curr);\t\treturn;\t&#125;\t/*\t * Ensure that a task that missed wakeup preemption by a\t * narrow margin doesn&#x27;t have to wait for a full slice.\t * This also mitigates buddy induced latencies under load.\t */\tif (delta_exec &lt; sysctl_sched_min_granularity)\t\treturn;  // 取出红黑树最小的进程\tse = __pick_first_entity(cfs_rq);\tdelta = curr-&gt;vruntime - se-&gt;vruntime;\tif (delta &lt; 0)\t\treturn;\tif (delta &gt; ideal_runtime)\t\tresched_curr(rq_of(cfs_rq));&#125;\n\n如果发现当前进程运行过长或者当前进程的 vruntime 大于红黑树中最小的进程的 vruntime，就会调用 resched_curr 标记当前进程为被抢占，而不是真正的抢占，而是打上一个标签 TIF_NEED_RESCHED\n另外一个可能抢占的场景就是当一个进程被唤醒的时候。当一个进程在等待 I&#x2F;O 当时候，会主动放弃 CPU，但是当 I&#x2F;O 到来的时候，进程往往会被唤醒。这时候是一个时机。当被唤醒的进程优先级高于 CPU 上的当前进程，就会触发抢占。\nflowchart LR\n\ntry_to_wake_up --&gt; ttwu_queue\nttwu_queue --&gt; ttwu_do_activate\nttwu_do_activate --&gt; ttwu_do_wakeup\nttwu_do_wakeup --&gt; check_preempt_curr\n\n到这里，只是把进程标记为被抢占，但是没有发生真正的抢占动作\n抢占时机真正的抢占时机，是让正在运行的进程有机会调用 schedule。\n对于用户态进程来讲，从系统调用返回的那个时候，就是一个被抢占的时机。\n对于内核态的执行中，被抢占的时机一般发生在 preempt_enable() 中。对于内核有些操作不能被中断，所以一般都会 preempt_disable() 关闭抢占，当再次打开就是一次抢占的机会。还有就是从中断返回的时候返回的也是内核态，这个时候也是一个执行抢占的时机。\n\n参考链接:\n\nhttps://mp.weixin.qq.com/s/HFQL5d-C24DKjQbYAQfl8g\nhttps://mp.weixin.qq.com/s?__biz&#x3D;Mzk0MjE3NDE0Ng&#x3D;&#x3D;&amp;mid&#x3D;2247501576&amp;idx&#x3D;1&amp;sn&#x3D;657e1c89d1007a935820d394d64f4035&amp;scene&#x3D;21#wechat_redirect\nhttps://zhuanlan.zhihu.com/p/79772089\nhttps://www.jianshu.com/p/0d90b92000c0\nhttps://segmentfault.com/a/1190000039367851?utm_source&#x3D;sf-similar-article\nhttps://www.cnblogs.com/chaojiyingxiong/p/15799617.html\nhttp://www.wowotech.net/process_management&#x2F;scheduler-history.html\nhttps://www.ibm.com/developerworks/cn/linux/l-cn-scheduler/index.html\nhttps://www.jianshu.com/p/673c9e4817a8\nhttps://blog.eastonman.com/blog/2021/02/cfs/\nhttps://www.kernel.org/doc/html/latest/scheduler/sched-design-CFS.html\nhttps://time.geekbang.org/column/intro/100024701?tab=catalog\n\n","categories":["OS"],"tags":["Linux"]},{"title":"使用 HEXO 构建 github 博客","url":"/2022/02/28/other/build-github-blog-with-hexo/","content":"Init blognpm install hexo-cli -ghexo init blogcd blognpm installhexo server\n\nThemes可以在 这里 查看自己喜欢的主题，我使用的是 keep, 这个主题(对移动端也进行适配了的)，如果喜欢的话可以根据 这个配置 配置成你想要的样式\nGithub Setting使用 Github Actions 自动部署 Hexo 博客, 这个就是我参考的一个文档，其中需要说明的一些点有如下的地方：\nssh-keygen使用 ssh-keygen 生成密钥对的时候不要输入密码\ndeploy最后的 workflow 文件里面的 npm hexo deploy 是需要把项目根目录的 _config.yml 文件里面的 deploy type 修改成这样：\ndeploy:  type: &#x27;git&#x27;  repo: &quot;git@github.com:champly/champly.github.io.git&quot;  branch: main  name: champly  email: champly@outlook.com\n\n同时还需要安装 hexo-deployer-git，要不然的话是不支持 git 类型，使用 npm install hexo-deployer-git 进行安装\nthemes config_config.theme.yml 这里面是我的配置，在 workflow 里面是有一个替换配置和导入 images 的过程\ncp _config.theme.yml themes/keep/_config.ymlcp images/* themes/keep/source/images/\n\n这里的主要逻辑就是通过 github actions 自动构建，然后通过配置证书，让 npm deploy 的时候可以直接推送到 main 分支，所以就需要把默认分支配置成 source，展示的分支配置成 main.\n其他的部分在上面的文档里面介绍的比较详细了，可以多看看.\n","categories":["Blog"],"tags":["Hexo","Next"]}]